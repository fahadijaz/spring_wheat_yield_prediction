{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "To do\n",
    "\n",
    "Check Time Series data\n",
    "\n",
    "Give a 2 page report to Sahameh about visualisation and indicate the data which has any problems, which is in consistant with the data from other dates\n",
    "Planting date for 2018 is given in master thesis\n",
    "Planting date for 2019 is May 05\n",
    "Planting date for 2020 and remaining data will be given by Sahameh in the week starrting from 16th November\n",
    "\n",
    "A lot of variations in MAT data, i.e. days to maturation.\n",
    "Some have number of days while others have dates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Must do normalisation of data before training\n",
    "\n",
    "Next step is Global Mix modelling\n",
    "Sahameh will share a paper from 2020 about using hyper spectral imaging, which will be basis of Mix Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue_median</th>\n",
       "      <th>green_median</th>\n",
       "      <th>red_median</th>\n",
       "      <th>red_edge_median</th>\n",
       "      <th>ndvi_median</th>\n",
       "      <th>nir_median</th>\n",
       "      <th>mtci_values</th>\n",
       "      <th>evi_values</th>\n",
       "      <th>grain_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034152</td>\n",
       "      <td>0.085196</td>\n",
       "      <td>0.045748</td>\n",
       "      <td>0.174878</td>\n",
       "      <td>0.883373</td>\n",
       "      <td>0.734195</td>\n",
       "      <td>4.331436</td>\n",
       "      <td>0.982070</td>\n",
       "      <td>431.698672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036060</td>\n",
       "      <td>0.084995</td>\n",
       "      <td>0.045884</td>\n",
       "      <td>0.168578</td>\n",
       "      <td>0.876906</td>\n",
       "      <td>0.692295</td>\n",
       "      <td>4.268514</td>\n",
       "      <td>0.952200</td>\n",
       "      <td>409.089032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036943</td>\n",
       "      <td>0.089985</td>\n",
       "      <td>0.045336</td>\n",
       "      <td>0.180775</td>\n",
       "      <td>0.888153</td>\n",
       "      <td>0.759447</td>\n",
       "      <td>4.272559</td>\n",
       "      <td>1.017603</td>\n",
       "      <td>372.860721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030649</td>\n",
       "      <td>0.072090</td>\n",
       "      <td>0.036053</td>\n",
       "      <td>0.151221</td>\n",
       "      <td>0.906785</td>\n",
       "      <td>0.739433</td>\n",
       "      <td>5.107437</td>\n",
       "      <td>1.018870</td>\n",
       "      <td>528.219355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.037586</td>\n",
       "      <td>0.085404</td>\n",
       "      <td>0.050601</td>\n",
       "      <td>0.167592</td>\n",
       "      <td>0.869756</td>\n",
       "      <td>0.713398</td>\n",
       "      <td>4.665330</td>\n",
       "      <td>0.954982</td>\n",
       "      <td>462.505958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9265</th>\n",
       "      <td>0.029813</td>\n",
       "      <td>0.115001</td>\n",
       "      <td>0.043041</td>\n",
       "      <td>0.235598</td>\n",
       "      <td>0.896805</td>\n",
       "      <td>0.778433</td>\n",
       "      <td>3.080369</td>\n",
       "      <td>1.906517</td>\n",
       "      <td>631.004782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9266</th>\n",
       "      <td>0.028875</td>\n",
       "      <td>0.116390</td>\n",
       "      <td>0.041759</td>\n",
       "      <td>0.235039</td>\n",
       "      <td>0.903283</td>\n",
       "      <td>0.792686</td>\n",
       "      <td>2.926750</td>\n",
       "      <td>1.949479</td>\n",
       "      <td>641.486148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9267</th>\n",
       "      <td>0.028778</td>\n",
       "      <td>0.119711</td>\n",
       "      <td>0.039235</td>\n",
       "      <td>0.255132</td>\n",
       "      <td>0.916507</td>\n",
       "      <td>0.884904</td>\n",
       "      <td>2.912663</td>\n",
       "      <td>2.181455</td>\n",
       "      <td>562.676888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>0.026632</td>\n",
       "      <td>0.106301</td>\n",
       "      <td>0.036410</td>\n",
       "      <td>0.221450</td>\n",
       "      <td>0.926392</td>\n",
       "      <td>0.858537</td>\n",
       "      <td>3.239740</td>\n",
       "      <td>2.120530</td>\n",
       "      <td>547.551879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9269</th>\n",
       "      <td>0.077920</td>\n",
       "      <td>0.154683</td>\n",
       "      <td>0.164282</td>\n",
       "      <td>0.236207</td>\n",
       "      <td>0.452900</td>\n",
       "      <td>0.440633</td>\n",
       "      <td>2.945026</td>\n",
       "      <td>0.917764</td>\n",
       "      <td>617.828008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9270 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      blue_median  green_median  red_median  red_edge_median  ndvi_median  \\\n",
       "0        0.034152      0.085196    0.045748         0.174878     0.883373   \n",
       "1        0.036060      0.084995    0.045884         0.168578     0.876906   \n",
       "2        0.036943      0.089985    0.045336         0.180775     0.888153   \n",
       "3        0.030649      0.072090    0.036053         0.151221     0.906785   \n",
       "4        0.037586      0.085404    0.050601         0.167592     0.869756   \n",
       "...           ...           ...         ...              ...          ...   \n",
       "9265     0.029813      0.115001    0.043041         0.235598     0.896805   \n",
       "9266     0.028875      0.116390    0.041759         0.235039     0.903283   \n",
       "9267     0.028778      0.119711    0.039235         0.255132     0.916507   \n",
       "9268     0.026632      0.106301    0.036410         0.221450     0.926392   \n",
       "9269     0.077920      0.154683    0.164282         0.236207     0.452900   \n",
       "\n",
       "      nir_median  mtci_values  evi_values  grain_yield  \n",
       "0       0.734195     4.331436    0.982070   431.698672  \n",
       "1       0.692295     4.268514    0.952200   409.089032  \n",
       "2       0.759447     4.272559    1.017603   372.860721  \n",
       "3       0.739433     5.107437    1.018870   528.219355  \n",
       "4       0.713398     4.665330    0.954982   462.505958  \n",
       "...          ...          ...         ...          ...  \n",
       "9265    0.778433     3.080369    1.906517   631.004782  \n",
       "9266    0.792686     2.926750    1.949479   641.486148  \n",
       "9267    0.884904     2.912663    2.181455   562.676888  \n",
       "9268    0.858537     3.239740    2.120530   547.551879  \n",
       "9269    0.440633     2.945026    0.917764   617.828008  \n",
       "\n",
       "[9270 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ml_df = pd.read_csv(\"ml_df.csv\")\n",
    "ml_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue_median</th>\n",
       "      <th>green_median</th>\n",
       "      <th>red_median</th>\n",
       "      <th>red_edge_median</th>\n",
       "      <th>ndvi_median</th>\n",
       "      <th>nir_median</th>\n",
       "      <th>mtci_values</th>\n",
       "      <th>evi_values</th>\n",
       "      <th>grain_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.130457</td>\n",
       "      <td>-0.157766</td>\n",
       "      <td>-0.395402</td>\n",
       "      <td>-0.293135</td>\n",
       "      <td>0.646189</td>\n",
       "      <td>0.900287</td>\n",
       "      <td>1.056244</td>\n",
       "      <td>0.550446</td>\n",
       "      <td>-0.813373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.029143</td>\n",
       "      <td>-0.162631</td>\n",
       "      <td>-0.392904</td>\n",
       "      <td>-0.412502</td>\n",
       "      <td>0.609372</td>\n",
       "      <td>0.685036</td>\n",
       "      <td>1.011221</td>\n",
       "      <td>0.455895</td>\n",
       "      <td>-0.996932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017724</td>\n",
       "      <td>-0.041961</td>\n",
       "      <td>-0.402930</td>\n",
       "      <td>-0.181387</td>\n",
       "      <td>0.673402</td>\n",
       "      <td>1.030015</td>\n",
       "      <td>1.014115</td>\n",
       "      <td>0.662924</td>\n",
       "      <td>-1.291056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.316480</td>\n",
       "      <td>-0.474694</td>\n",
       "      <td>-0.572730</td>\n",
       "      <td>-0.741371</td>\n",
       "      <td>0.779473</td>\n",
       "      <td>0.927199</td>\n",
       "      <td>1.611503</td>\n",
       "      <td>0.666935</td>\n",
       "      <td>-0.029760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051910</td>\n",
       "      <td>-0.152734</td>\n",
       "      <td>-0.306641</td>\n",
       "      <td>-0.431167</td>\n",
       "      <td>0.568668</td>\n",
       "      <td>0.793447</td>\n",
       "      <td>1.295158</td>\n",
       "      <td>0.464703</td>\n",
       "      <td>-0.563261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9265</th>\n",
       "      <td>-0.360836</td>\n",
       "      <td>0.562950</td>\n",
       "      <td>-0.444914</td>\n",
       "      <td>0.857362</td>\n",
       "      <td>0.722656</td>\n",
       "      <td>1.127553</td>\n",
       "      <td>0.161057</td>\n",
       "      <td>3.476724</td>\n",
       "      <td>0.804715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9266</th>\n",
       "      <td>-0.410676</td>\n",
       "      <td>0.596548</td>\n",
       "      <td>-0.468366</td>\n",
       "      <td>0.846763</td>\n",
       "      <td>0.759540</td>\n",
       "      <td>1.200775</td>\n",
       "      <td>0.051136</td>\n",
       "      <td>3.612715</td>\n",
       "      <td>0.889809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9267</th>\n",
       "      <td>-0.415830</td>\n",
       "      <td>0.676855</td>\n",
       "      <td>-0.514521</td>\n",
       "      <td>1.227471</td>\n",
       "      <td>0.834824</td>\n",
       "      <td>1.674524</td>\n",
       "      <td>0.041057</td>\n",
       "      <td>4.347021</td>\n",
       "      <td>0.249988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9268</th>\n",
       "      <td>-0.529777</td>\n",
       "      <td>0.352573</td>\n",
       "      <td>-0.566194</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>0.891095</td>\n",
       "      <td>1.539072</td>\n",
       "      <td>0.275092</td>\n",
       "      <td>4.154168</td>\n",
       "      <td>0.127194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9269</th>\n",
       "      <td>2.193560</td>\n",
       "      <td>1.522515</td>\n",
       "      <td>1.772638</td>\n",
       "      <td>0.868894</td>\n",
       "      <td>-1.804494</td>\n",
       "      <td>-0.607827</td>\n",
       "      <td>0.064213</td>\n",
       "      <td>0.346891</td>\n",
       "      <td>0.697738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9270 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      blue_median  green_median  red_median  red_edge_median  ndvi_median  \\\n",
       "0       -0.130457     -0.157766   -0.395402        -0.293135     0.646189   \n",
       "1       -0.029143     -0.162631   -0.392904        -0.412502     0.609372   \n",
       "2        0.017724     -0.041961   -0.402930        -0.181387     0.673402   \n",
       "3       -0.316480     -0.474694   -0.572730        -0.741371     0.779473   \n",
       "4        0.051910     -0.152734   -0.306641        -0.431167     0.568668   \n",
       "...           ...           ...         ...              ...          ...   \n",
       "9265    -0.360836      0.562950   -0.444914         0.857362     0.722656   \n",
       "9266    -0.410676      0.596548   -0.468366         0.846763     0.759540   \n",
       "9267    -0.415830      0.676855   -0.514521         1.227471     0.834824   \n",
       "9268    -0.529777      0.352573   -0.566194         0.589286     0.891095   \n",
       "9269     2.193560      1.522515    1.772638         0.868894    -1.804494   \n",
       "\n",
       "      nir_median  mtci_values  evi_values  grain_yield  \n",
       "0       0.900287     1.056244    0.550446    -0.813373  \n",
       "1       0.685036     1.011221    0.455895    -0.996932  \n",
       "2       1.030015     1.014115    0.662924    -1.291056  \n",
       "3       0.927199     1.611503    0.666935    -0.029760  \n",
       "4       0.793447     1.295158    0.464703    -0.563261  \n",
       "...          ...          ...         ...          ...  \n",
       "9265    1.127553     0.161057    3.476724     0.804715  \n",
       "9266    1.200775     0.051136    3.612715     0.889809  \n",
       "9267    1.674524     0.041057    4.347021     0.249988  \n",
       "9268    1.539072     0.275092    4.154168     0.127194  \n",
       "9269   -0.607827     0.064213    0.346891     0.697738  \n",
       "\n",
       "[9270 rows x 9 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ml_df_std\n",
    "ml_df_std_yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ml_df_std\n",
    "\n",
    "# df = ml_df_std_yield\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#==============================================================================\n",
    "# Split dataframe into data and target\n",
    "#==============================================================================\n",
    "\n",
    "X = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,-1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[431.698671726755],\n",
       "       [409.089032258064],\n",
       "       [372.860721062619],\n",
       "       ...,\n",
       "       [562.6768880455409],\n",
       "       [547.5518785578748],\n",
       "       [617.828007590133]], dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#==============================================================================\n",
    "# Create separate train/test splits from Main data\n",
    "#==============================================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# #==============================================================================\n",
    "# # Scale features using StandardScaler class in scikit-learn\n",
    "# #==============================================================================\n",
    "\n",
    "# # Initialise standard scaler and compute mean and STD from training data\n",
    "# sc = StandardScaler()\n",
    "# sc.fit(X_train)\n",
    "\n",
    "# # Transform (standardise) both X_train and X_test with mean and STD from\n",
    "# # training data\n",
    "# X_train_std = sc.transform(X_train)\n",
    "# X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#==============================================================================\n",
    "# Defining the function to vaiidate the model with the test data and\n",
    "# get the results from regression evaluation metrices in sklearn\n",
    "#==============================================================================\n",
    "pred = []\n",
    "accuracy = []\n",
    "\n",
    "\n",
    "def test_data_regression(model, y_pred, y_test, target_state= 'actual', comments= 'no comments'):\n",
    "    pred = []\n",
    "    accuracy = []\n",
    "\n",
    "    if target_state == 'transformed':\n",
    "        y_pred = sc.inverse_transform(y_pred)\n",
    "        y_test = sc.inverse_transform(y_test)\n",
    "    #==============================================================================\n",
    "    # Compute performance\n",
    "    #==============================================================================\n",
    "\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(mae, ' mean_absolute_error')\n",
    "    accuracy.append(mae)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mse = mean_squared_error(y_test, y_pred, squared=True)\n",
    "    print(mse, ' mean_squared_error')\n",
    "    accuracy.append(mse)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(rmse, ' root_mean_squared_error')\n",
    "    accuracy.append(rmse)\n",
    "\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(r2, ' r2_score')\n",
    "    accuracy.append(r2)\n",
    "\n",
    "    # Print accuracy computed from predictions on the test set\n",
    "    print(accuracy)\n",
    "\n",
    "    #==============================================================================\n",
    "    # Append Results\n",
    "    #==============================================================================\n",
    "    results = []\n",
    "    import datetime\n",
    "    datetime = datetime.datetime.now()\n",
    "    results.append((model, 'MAE = {}'.format(mae), 'MSE = {}'.format(mse),\n",
    "                    'RMSE = {}'.format(rmse), 'R2 = {}'.format(r2),\n",
    "                    'List = {}'.format(accuracy), datetime, target_state, comments))\n",
    "\n",
    "    pd.DataFrame(np.asarray(results)).to_csv('results.csv',\n",
    "                                             mode='a',\n",
    "                                             header=None)\n",
    "    pred.extend(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757.3234479605273  mean_absolute_error\n",
      "19879011.9912847  mean_squared_error\n",
      "4458.58856492553  root_mean_squared_error\n",
      "-1285.5287015224958  r2_score\n",
      "[757.3234479605273, 19879011.9912847, 4458.58856492553, -1285.5287015224958]\n",
      "Wall time: 8.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "model = GaussianProcessRegressor()\n",
    "model\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "test_data_regression(model, y_pred, y_test, 'actual', 'transformed target')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1285.7837651454133  r2_score\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(r2, ' r2_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=250, min_samples_split=5, n_estimators=1000,\n",
       "                      n_jobs=-1, random_state=0)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=1000,\n",
    "                              max_depth=250,\n",
    "                              min_samples_split=5,\n",
    "                              random_state=0,\n",
    "                              n_jobs=-1)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Reshaping y from column ventor to 1D array\n",
    "y = np.reshape(y, (1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# create target scaler object\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaler.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80527278],\n",
       "       [0.87110207],\n",
       "       [0.48169135],\n",
       "       ...,\n",
       "       [0.86413493],\n",
       "       [0.80114103],\n",
       "       [0.70831504]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40024303],\n",
       "       [0.75600504],\n",
       "       [0.68143201],\n",
       "       ...,\n",
       "       [0.77390239],\n",
       "       [0.49967252],\n",
       "       [0.07300864]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# transform target variables\n",
    "y_train = target_scaler.transform(y_train)\n",
    "y_test = target_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-257-d768f88d541e>:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_train, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=250, min_samples_split=5, n_estimators=1000,\n",
       "                      n_jobs=-1, random_state=0)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# invert transform on predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38058973],\n",
       "       [0.72374907],\n",
       "       [0.71717564],\n",
       "       ...,\n",
       "       [0.75016846],\n",
       "       [0.60480157],\n",
       "       [0.21767988]])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "y_pred = np.reshape(y_pred, (-1, 1))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[351.61257326],\n",
       "       [565.63091115],\n",
       "       [561.53125689],\n",
       "       ...,\n",
       "       [582.10790565],\n",
       "       [491.44686838],\n",
       "       [250.01050887]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = target_scaler.inverse_transform(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_test = target_scaler.inverse_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.17863178170342  mean_absolute_error\n",
      "3741.77485989703  mean_squared_error\n",
      "61.17004871583012  root_mean_squared_error\n",
      "0.7578400398368218  r2_score\n",
      "[44.17863178170342, 3741.77485989703, 61.17004871583012, 0.7578400398368218]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data_regression(model, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40024303],\n",
       "       [0.75600504],\n",
       "       [0.68143201],\n",
       "       ...,\n",
       "       [0.77390239],\n",
       "       [0.49967252],\n",
       "       [0.07300864]])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "y_test = target_scaler.inverse_transform(y_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "\n",
    "# define the target transform wrapper\n",
    "wrapped_model = TransformedTargetRegressor(regressor=model, transformer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.18129274951714  mean_absolute_error\n",
      "3742.084454236362  mean_squared_error\n",
      "61.17257926748194  root_mean_squared_error\n",
      "0.7578200035290036  r2_score\n",
      "[44.18129274951714, 3742.084454236362, 61.17257926748194, 0.7578200035290036]\n"
     ]
    }
   ],
   "source": [
    "# use the target transform wrapper\n",
    "\n",
    "wrapped_model.fit(X_train_std, y_train)\n",
    "y_pred = wrapped_model.predict(X_test_std)\n",
    "test_data_regression(wrapped_model, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-66-f732706e2812>:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_train_std, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.18129274951714  mean_absolute_error\n",
      "3742.084454236362  mean_squared_error\n",
      "61.17257926748194  root_mean_squared_error\n",
      "0.7578200035290036  r2_score\n",
      "[44.18129274951714, 3742.084454236362, 61.17257926748194, 0.7578200035290036]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=1000,\n",
    "                              max_depth=250,\n",
    "                              min_samples_split=5,\n",
    "                              random_state=0,\n",
    "                              n_jobs=-1)\n",
    "model.fit(X_train_std, y_train)\n",
    "y_pred = model.predict(X_test_std)\n",
    "test_data_regression(model, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.65415189943113  mean_absolute_error\n",
      "10483.149184427324  mean_squared_error\n",
      "102.38725108345923  root_mean_squared_error\n",
      "0.3215521820691706  r2_score\n",
      "[74.65415189943113, 10483.149184427324, 102.38725108345923, 0.3215521820691706]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-67-c00bc0704170>:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_train_std, y_train)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=10,\n",
    "                              max_depth=1,\n",
    "                              min_samples_split=10,\n",
    "                              random_state=0,\n",
    "                              n_jobs=-1)\n",
    "model.fit(X_train_std, y_train)\n",
    "y_pred = model.predict(X_test_std)\n",
    "test_data_regression(model, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.50208431237094  mean_absolute_error\n",
      "7938.921119041205  mean_squared_error\n",
      "89.10062356146115  root_mean_squared_error\n",
      "0.4862093808662402  r2_score\n",
      "[65.50208431237094, 7938.921119041205, 89.10062356146115, 0.4862093808662402]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "model = PLSRegression(n_components=5)\n",
    "model.fit(X_train_std, y_train)\n",
    "y_pred = model.predict(X_test_std)\n",
    "test_data_regression(model, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-128.07574617410896\n",
      "{'randomforestregressor__max_depth': 3, 'randomforestregressor__min_samples_split': 5, 'randomforestregressor__n_estimators': 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:335: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipe_line = make_pipeline(RandomForestRegressor())\n",
    "\n",
    "# Define ranges of parameter values:\n",
    "param_range  = list(range(1,200))                   # For n_components\n",
    "param_range2 = list(range(1, 20, 1))          # For max_iter\n",
    "param_range3 = list(range(1, 15,1))                   # For max_depth\n",
    "param_range4 = [x/10 for x in list(range(0, 10))]   # For learning_rate\n",
    "param_range5  = list(range(5,20))                   # For n_components\n",
    "\n",
    "\n",
    "# estimator.get_params().keys()\n",
    "# pipe_line.get_params().keys()\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_line, \n",
    "                  param_grid=[{'randomforestregressor__n_estimators': param_range2,\n",
    "                              'randomforestregressor__max_depth': param_range3,\n",
    "                               'randomforestregressor__min_samples_split': param_range3}], \n",
    "                  scoring='neg_root_mean_squared_error', \n",
    "                  cv=3,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs = gs.fit(X, y)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "\n",
    "# # Inspect AUC of parameter grid combinations\n",
    "# for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n",
    "#     print(\"%0.3f +/- %0.2f %r\"\n",
    "#           % (grid.cv_results_['mean_test_score'][r], \n",
    "#              grid.cv_results_['std_test_score'][r] / 2.0, \n",
    "#              grid.cv_results_['params'][r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GrainYield    223.1\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    print(y_pred[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import cv2\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Neural networks\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Training\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define eval metric and loss function (DICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Metric used in the competition\n",
    "\"\"\" \n",
    "# Defining the dice_coef function\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "# Defining the dice_loss function\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-32615f7b7f52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mNN_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NN_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-9dc7236c1b27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mNN_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mNN_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mNN_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NN_model' is not defined"
     ]
    }
   ],
   "source": [
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a checkpoint callback :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third : Train the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.fit(train, target, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wights file of the best model :\n",
    "wights_file = 'Weights-478--18738.19831.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the validation loss of the best model is 18738.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Fourth : Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(prediction, sub_name):\n",
    "  my_submission = pd.DataFrame({'Id':pd.read_csv('test.csv').Id,'SalePrice':prediction})\n",
    "  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n",
    "  print('A submission file has been made')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = NN_model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(predictions[:,0],'submission(NN).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Not bad at all, with some more preprocessing, and more training, we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Create U-net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Version of U-Net with dropout and size preservation (padding= 'same')\n",
    "\"\"\" \n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def get_deep_unet(input_img, n_filters = 16, dropout = 0.1, batchnorm = True, n_classes = 1, growth_factor=2, upconv=True):\n",
    "\n",
    "    inputs = input_img\n",
    "    #inputs = BatchNormalization()(inputs)\n",
    "    \n",
    "    # Creating deep convnets using Conv2D and MaxPooling2D in each convolutional layer\n",
    "    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    #pool1 = Dropout(droprate)(pool1)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    # Applying batch normalization to each layer to be able to use deep convnets\n",
    "    pool1 = BatchNormalization()(pool1)\n",
    "    # Second Covnet\n",
    "    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(dropout)(pool2)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    # Applying batch normalization\n",
    "    pool2 = BatchNormalization()(pool2)\n",
    "    # Third Covnet\n",
    "    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(dropout)(pool3)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    # Applying batch normalization\n",
    "    pool3 = BatchNormalization()(pool3)\n",
    "    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_0)\n",
    "    pool4_1 = MaxPooling2D(pool_size=(2, 2))(conv4_0)\n",
    "    pool4_1 = Dropout(dropout)(pool4_1)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    # Applying batch normalization\n",
    "    pool4_1 = BatchNormalization()(pool4_1)\n",
    "    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_1)\n",
    "    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2, 2))(conv4_1)\n",
    "    pool4_2 = Dropout(dropout)(pool4_2)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_2)\n",
    "    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    n_filters //= growth_factor\n",
    "    # Using image upsampling by either Conv2DTranspose or UpSampling2D\n",
    "    if upconv:\n",
    "        up6_1 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv5), conv4_1])\n",
    "    else:\n",
    "        up6_1 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4_1])\n",
    "    # Applying batch normalization\n",
    "    up6_1 = BatchNormalization()(up6_1)\n",
    "    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_1)\n",
    "    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_1)\n",
    "    conv6_1 = Dropout(dropout)(conv6_1)\n",
    "\n",
    "    # Using image upsampling by either Conv2DTranspose or UpSampling2D\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up6_2 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_1), conv4_0])\n",
    "    else:\n",
    "        up6_2 = concatenate([UpSampling2D(size=(2, 2))(conv6_1), conv4_0])\n",
    "    up6_2 = BatchNormalization()(up6_2)\n",
    "    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_2)\n",
    "    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_2)\n",
    "    conv6_2 = Dropout(dropout)(conv6_2)\n",
    "\n",
    "    # Using image upsampling by either Conv2DTranspose or UpSampling2D\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up7 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_2), conv3])\n",
    "    else:\n",
    "        up7 = concatenate([UpSampling2D(size=(2, 2))(conv6_2), conv3])\n",
    "    # Applying batch normalization\n",
    "    up7 = BatchNormalization()(up7)\n",
    "    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = Dropout(dropout)(conv7)\n",
    "\n",
    "    # Using image upsampling by either Conv2DTranspose or UpSampling2D\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up8 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv7), conv2])\n",
    "    else:\n",
    "        up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2])\n",
    "    # Applying batch normalization\n",
    "    up8 = BatchNormalization()(up8)\n",
    "    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = Dropout(dropout)(conv8)\n",
    "\n",
    "    # Using image upsampling by either Conv2DTranspose or UpSampling2D\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up9 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv8), conv1])\n",
    "    else:\n",
    "        up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1])\n",
    "    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(n_classes, (1, 1), activation='sigmoid')(conv9)\n",
    "    \n",
    "    # Creating model and feeding the input image and the final convolutional layer into it\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input to `.fit()` should have rank 4. Got array with shape: (67, 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-639feb388496>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Fit DataGen instance to X_train (Only needed for standardization etc.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtrain_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Create flow for training images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, augment, rounds, seed)\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m             raise ValueError('Input to `.fit()` should have rank 4. '\n\u001b[1;32m--> 936\u001b[1;33m                              'Got array with shape: ' + str(x.shape))\n\u001b[0m\u001b[0;32m    937\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannel_axis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m             warnings.warn(\n",
      "\u001b[1;31mValueError\u001b[0m: Input to `.fit()` should have rank 4. Got array with shape: (67, 8)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data augmentation for training dataset\n",
    "\"\"\"\n",
    "\n",
    "# Define batch size for augmentation\n",
    "batch_size = 64\n",
    "# Define random seed\n",
    "seed = 123\n",
    "\n",
    "# Dict containing augmentation variables\n",
    "data_gen_args = dict(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    rotation_range=90,\n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "# Instantiate DataGenerator class with variables from dict\n",
    "train_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Fit DataGen instance to X_train (Only needed for standardization etc.)\n",
    "train_datagen.fit(X_train)\n",
    "\n",
    "# Create flow for training images\n",
    "image_train_generator = train_datagen.flow(\n",
    "    X_train,\n",
    "    None,\n",
    "    seed=seed,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# Create separate flow for training masks\n",
    "mask_train_generator = train_datagen.flow(\n",
    "    y_train,\n",
    "    None,\n",
    "    seed=seed,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# Combine generators into one which yields image and masks\n",
    "train_generator = zip(image_train_generator, mask_train_generator)\n",
    "\"\"\"\n",
    "Model creation and compilation\n",
    "\"\"\"\n",
    "\n",
    "# Define input class with correct shape\n",
    "input_img = Input(shape=(128, 128, 4))\n",
    "\n",
    "# Number of filters in the U-net\n",
    "n_filters = 64\n",
    "\n",
    "# Instantiate U-net without dropout with batchnorm\n",
    "model = get_deep_unet(input_img,\n",
    "                      n_filters=n_filters,\n",
    "                      dropout=0.0,\n",
    "                      batchnorm=True,\n",
    "                      n_classes=1)\n",
    "\n",
    "# Compile model using \"Adam\" with custom loss function and metrics\n",
    "model.compile(optimizer='adam', loss=dice_loss, metrics=[dice_coef])\n",
    "\"\"\"\n",
    "Callbacks\n",
    "\"\"\"\n",
    "\n",
    "# Early stopping for saving compute time\n",
    "earlystopping = EarlyStopping(monitor='val_dice_coef',\n",
    "                              verbose=1,\n",
    "                              min_delta=0.01,\n",
    "                              patience=3,\n",
    "                              mode='max')\n",
    "\n",
    "# Saving best models\n",
    "model_path = '/content/drive/My Drive/Skole/CA2/tmp/unet.model'\n",
    "checkpoint = ModelCheckpoint(model_path,\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1,\n",
    "                             mode='min')\n",
    "\n",
    "# Reduce learning rate to improve accuracy on plateaus\n",
    "redlr = ReduceLROnPlateau(factor=0.1,\n",
    "                          patience=1,\n",
    "                          min_lr=1e-08,\n",
    "                          verbose=1,\n",
    "                          monitor='val_loss',\n",
    "                          mode='min'),\n",
    "\n",
    "# Collect callbacks in list\n",
    "callbacks_list = [redlr, earlystopping, checkpoint]\n",
    "\"\"\"\n",
    "Model training\n",
    "\"\"\"\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=(X_train.shape[0] // n_filters),\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    validation_steps=(X_val.shape[0] // n_filters),\n",
    "                    epochs=4,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from xgboost import XGBRegressor"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
