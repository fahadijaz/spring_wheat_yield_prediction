{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather and Genomics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Half datasets, with separate files for east and west subplots have been merged manually in excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:25:15.060092Z",
     "start_time": "2021-10-02T22:25:13.268068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "\n",
    "# Dictionaries\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Iterate in loops\n",
    "from itertools import zip_longest\n",
    "\n",
    "# Simpsons integration\n",
    "from numpy import trapz\n",
    "from scipy.integrate import simps\n",
    "\n",
    "# Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To display df nicely in loops\n",
    "from IPython.display import display \n",
    "# display(df1.head()) \n",
    "# display(df2.head())\n",
    "\n",
    "# Display rows and columns Pandas\n",
    "pd.options.display.max_columns = 100\n",
    "pd.set_option('display.max_rows',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:25:15.076032Z",
     "start_time": "2021-10-02T22:25:15.062070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\fahad\\\\MegaSync\\\\NMBU\\\\GitHub\\\\vPheno'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints the current working directory\n",
    "os.getcwd()\n",
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Username folder to make general path for multi PC use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:25:15.091861Z",
     "start_time": "2021-10-02T22:25:15.078028Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fahad', 'C:/Users/fahad/')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username = str(os.getcwd()).split('\\\\')[2]\n",
    "user_path = r'C:/Users/'+username+'/'\n",
    "username, user_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:25:15.107816Z",
     "start_time": "2021-10-02T22:25:15.094852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Graminor_2019_all.csv',\n",
       " 'Graminor_2020_all.csv',\n",
       " 'Masbasis_2019_all.csv',\n",
       " 'Masbasis_2020_all_lodg.csv',\n",
       " 'Robot_2020_all.csv',\n",
       " 'Staur_2019_all.csv',\n",
       " 'Staur_2020_all_lodg.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_path = r'./Data/'\n",
    "path = r'./Data/renamed_merged/'\n",
    "export_path = './Data/results/'\n",
    "\n",
    "# Create export_path folder if not exists already\n",
    "os.makedirs(path, exist_ok=True)\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Creating list of complete files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:25:15.123773Z",
     "start_time": "2021-10-02T22:25:15.108813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 files found in the directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Graminor_2019_all.csv',\n",
       " 'Graminor_2020_all.csv',\n",
       " 'Masbasis_2019_all.csv',\n",
       " 'Masbasis_2020_all_lodg.csv',\n",
       " 'Robot_2020_all.csv',\n",
       " 'Staur_2019_all.csv',\n",
       " 'Staur_2020_all_lodg.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of all files in directory tree at given path\n",
    "\n",
    "files_with_address = []\n",
    "files_list = []\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "    files_with_address += [os.path.join(dirpath, file) for file in filenames]\n",
    "    files_list.extend(filenames)\n",
    "    \n",
    "print(len(files_with_address), 'files found in the directory')\n",
    "# files_with_address\n",
    "files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Checking/control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "### Check for duplicate filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:25:15.139730Z",
     "start_time": "2021-10-02T22:25:15.125768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files are : 7\n",
      "Number of unique file names are: 7\n",
      "There is/are 0 duplicate file name/names.\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files are :', len(files_list))\n",
    "\n",
    "print('Number of unique file names are:', len(set(files_list)))\n",
    "\n",
    "print('There is/are', len(files_list) - len(set(files_list)),'duplicate file name/names.')\n",
    "if len(files_list) - len(set(files_list)) > 0:\n",
    "    raise NameError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data files to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:25:15.640776Z",
     "start_time": "2021-10-02T22:25:15.140728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graminor_2019_all ===== (600, 378)\n",
      "Graminor_2020_all ===== (400, 378)\n",
      "Masbasis_2019_all ===== (528, 278)\n",
      "Masbasis_2020_all_lodg ===== (659, 416)\n",
      "Robot_2020_all ===== (96, 484)\n",
      "Staur_2019_all ===== (1328, 346)\n",
      "Staur_2020_all_lodg ===== (1504, 209)\n",
      "Wall time: 492 ms\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "%%time\n",
    "\n",
    "all_df = []\n",
    "for data in files_with_address:\n",
    "    file_name = os.path.splitext(os.path.basename(data))[0]\n",
    "\n",
    "    # Replce all invalid characters in the name\n",
    "    file_name = file_name.replace(\" \", \"_\")\n",
    "    file_name = file_name.replace(\"-\", \"_\")\n",
    "    file_name = file_name.replace(\")\", \"\")\n",
    "    file_name = file_name.replace(\"(\", \"\")\n",
    "    df_name = file_name.replace(\".\", \"\")\n",
    "    # Test: Check if the same date is already present in the current dict key\n",
    "    if df_name in all_df:\n",
    "        print(f'A file with the same name {df_name} has already been imported. \\n Please check if there is duplication of data.')\n",
    "        raise NameError\n",
    "    all_df.append(df_name)\n",
    "\n",
    "    locals()[df_name] = pd.read_csv(data, index_col=False)\n",
    "    print(df_name, '=====', locals()[df_name].shape)\n",
    "# all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:25:15.655737Z",
     "start_time": "2021-10-02T22:25:15.642771Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total imported 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Graminor_2019_all',\n",
       " 'Graminor_2020_all',\n",
       " 'Masbasis_2019_all',\n",
       " 'Masbasis_2020_all_lodg',\n",
       " 'Robot_2020_all',\n",
       " 'Staur_2019_all',\n",
       " 'Staur_2020_all_lodg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Total imported {len(all_df)}')\n",
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding yield columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:25:15.686150Z",
     "start_time": "2021-10-02T22:25:15.656735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"GrainYield\" column in Graminor_2019_all is the yield column\n",
      " as it contains the text \"GrainYield\". It is located at location 6\n",
      "\"Name\" column in Graminor_2019_all is the yield column\n",
      " as it contains the text \"Name\". It is located at location 7\n",
      "\"Pedigree\" column in Graminor_2019_all is the yield column\n",
      " as it contains the text \"Pedigree\". It is located at location 8\n",
      "\"GrainYield\" column in Graminor_2020_all is the yield column\n",
      " as it contains the text \"GrainYield\". It is located at location 6\n",
      "\"Name\" column in Graminor_2020_all is the yield column\n",
      " as it contains the text \"Name\". It is located at location 7\n",
      "\"Pedigree\" column in Graminor_2020_all is the yield column\n",
      " as it contains the text \"Pedigree\". It is located at location 8\n",
      "\"GrainYield\" column in Masbasis_2019_all is the yield column\n",
      " as it contains the text \"GrainYield\". It is located at location 6\n",
      "\"Name\" column in Masbasis_2019_all is the yield column\n",
      " as it contains the text \"Name\". It is located at location 7\n",
      "\"Line\" column in Masbasis_2019_all is the yield column\n",
      " as it contains the text \"Line\". It is located at location 8\n",
      "\"Days2Heading\" column in Masbasis_2019_all is the yield column\n",
      " as it contains the text \"Days2Heading\". It is located at location 9\n",
      "\"Days2Maturity\" column in Masbasis_2019_all is the yield column\n",
      " as it contains the text \"Days2Maturity\". It is located at location 10\n",
      "\"GrainYield\" column in Masbasis_2020_all_lodg is the yield column\n",
      " as it contains the text \"GrainYield\". It is located at location 6\n",
      "\"Name\" column in Masbasis_2020_all_lodg is the yield column\n",
      " as it contains the text \"Name\". It is located at location 7\n",
      "\"Line\" column in Masbasis_2020_all_lodg is the yield column\n",
      " as it contains the text \"Line\". It is located at location 8\n",
      "\"Maturity_Date\" column in Masbasis_2020_all_lodg is the yield column\n",
      " as it contains the text \"Maturity_Date\". It is located at location 9\n",
      "\"Days2Heading\" column in Masbasis_2020_all_lodg is the yield column\n",
      " as it contains the text \"Days2Heading\". It is located at location 10\n",
      "\"Days2Maturity\" column in Masbasis_2020_all_lodg is the yield column\n",
      " as it contains the text \"Days2Maturity\". It is located at location 11\n",
      "\"Lodging\" column in Masbasis_2020_all_lodg is the yield column\n",
      " as it contains the text \"Lodging\". It is located at location 12\n",
      "\"GrainYield\" column in Robot_2020_all is the yield column\n",
      " as it contains the text \"GrainYield\". It is located at location 6\n",
      "\"Name\" column in Robot_2020_all is the yield column\n",
      " as it contains the text \"Name\". It is located at location 7\n",
      "\"CodeName\" column in Robot_2020_all is the yield column\n",
      " as it contains the text \"CodeName\". It is located at location 8\n",
      "\"Heading_Date\" column in Robot_2020_all is the yield column\n",
      " as it contains the text \"Heading_Date\". It is located at location 9\n",
      "\"Maturity_Date\" column in Robot_2020_all is the yield column\n",
      " as it contains the text \"Maturity_Date\". It is located at location 10\n",
      "\"Days2Heading\" column in Robot_2020_all is the yield column\n",
      " as it contains the text \"Days2Heading\". It is located at location 11\n",
      "\"Days2Maturity\" column in Robot_2020_all is the yield column\n",
      " as it contains the text \"Days2Maturity\". It is located at location 12\n",
      "\"GrainYield\" column in Staur_2019_all is the yield column\n",
      " as it contains the text \"GrainYield\". It is located at location 6\n",
      "\"Name\" column in Staur_2019_all is the yield column\n",
      " as it contains the text \"Name\". It is located at location 7\n",
      "\"Line\" column in Staur_2019_all is the yield column\n",
      " as it contains the text \"Line\". It is located at location 8\n",
      "\"Days2Heading\" column in Staur_2019_all is the yield column\n",
      " as it contains the text \"Days2Heading\". It is located at location 9\n",
      "\"Days2Maturity\" column in Staur_2019_all is the yield column\n",
      " as it contains the text \"Days2Maturity\". It is located at location 10\n",
      "\"GrainYield\" column in Staur_2020_all_lodg is the yield column\n",
      " as it contains the text \"GrainYield\". It is located at location 6\n",
      "\"Name\" column in Staur_2020_all_lodg is the yield column\n",
      " as it contains the text \"Name\". It is located at location 7\n",
      "\"Pedigree\" column in Staur_2020_all_lodg is the yield column\n",
      " as it contains the text \"Pedigree\". It is located at location 8\n",
      "\"Lodging\" column in Staur_2020_all_lodg is the yield column\n",
      " as it contains the text \"Lodging\". It is located at location 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'GrainYield_Graminor_2019_all': 6,\n",
       " 'Name_Graminor_2019_all': 7,\n",
       " 'Pedigree_Graminor_2019_all': 8,\n",
       " 'GrainYield_Graminor_2020_all': 6,\n",
       " 'Name_Graminor_2020_all': 7,\n",
       " 'Pedigree_Graminor_2020_all': 8,\n",
       " 'GrainYield_Masbasis_2019_all': 6,\n",
       " 'Name_Masbasis_2019_all': 7,\n",
       " 'Line_Masbasis_2019_all': 8,\n",
       " 'Days2Heading_Masbasis_2019_all': 9,\n",
       " 'Days2Maturity_Masbasis_2019_all': 10,\n",
       " 'GrainYield_Masbasis_2020_all_lodg': 6,\n",
       " 'Name_Masbasis_2020_all_lodg': 7,\n",
       " 'Line_Masbasis_2020_all_lodg': 8,\n",
       " 'Maturity_Date_Masbasis_2020_all_lodg': 9,\n",
       " 'Days2Heading_Masbasis_2020_all_lodg': 10,\n",
       " 'Days2Maturity_Masbasis_2020_all_lodg': 11,\n",
       " 'Lodging_Masbasis_2020_all_lodg': 12,\n",
       " 'GrainYield_Robot_2020_all': 6,\n",
       " 'Name_Robot_2020_all': 7,\n",
       " 'CodeName_Robot_2020_all': 8,\n",
       " 'Heading_Date_Robot_2020_all': 9,\n",
       " 'Maturity_Date_Robot_2020_all': 10,\n",
       " 'Days2Heading_Robot_2020_all': 11,\n",
       " 'Days2Maturity_Robot_2020_all': 12,\n",
       " 'GrainYield_Staur_2019_all': 6,\n",
       " 'Name_Staur_2019_all': 7,\n",
       " 'Line_Staur_2019_all': 8,\n",
       " 'Days2Heading_Staur_2019_all': 9,\n",
       " 'Days2Maturity_Staur_2019_all': 10,\n",
       " 'GrainYield_Staur_2020_all_lodg': 6,\n",
       " 'Name_Staur_2020_all_lodg': 7,\n",
       " 'Pedigree_Staur_2020_all_lodg': 8,\n",
       " 'Lodging_Staur_2020_all_lodg': 9}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ToDo: Add check for duplicate columns in the df\n",
    "\n",
    "general_col_names = ['Plot_ID', 'Blue', 'Green', 'Red', 'RedEdge', 'NIR']\n",
    "\n",
    "base_indices = ['Blue', 'Green', 'Red', 'RedEdge', 'NIR']\n",
    "\n",
    "spectral_indices = ['NDVI', 'MTCI', 'DVI', 'GDVI', 'MTCI_CI', 'EXG', 'EXGR', 'RDVI',\n",
    "                    'TDVI', 'GNDVI', 'NDRE', 'SCCI', 'EVI', 'TVI', 'VARI', 'GARI',\n",
    "                    'GCI', 'GLI', 'NLI', 'MNLI', 'SAVI', 'GSAVI', 'OSAVI', 'GOSAVI',\n",
    "                    'MSAVI2', 'MSR', 'GRVI', 'WDRVI', 'SR']\n",
    "# list_agg_df\n",
    "yield_cols = ['GrainYield', 'Name', 'CodeName', 'Pedigree', 'Line', 'Heading_Date',\n",
    "              'Maturity_Date', 'Days2Heading', 'Days2Maturity', 'Lodging']\n",
    "\n",
    "id_cols_new = ['Plot_ID']\n",
    "\n",
    "# Counter for location of column in columns list\n",
    "\n",
    "# Dict for saving the name and location of the yield column/s\n",
    "loc_yield_cols = {}\n",
    "for df in all_df:\n",
    "    loc = 0\n",
    "    for cols in locals()[df].columns.tolist():\n",
    "        for y_col in yield_cols:\n",
    "            if not cols.find(y_col):\n",
    "                loc_yield_cols[cols+'_'+df] = loc\n",
    "                print(f'\\\"{cols}\\\" column in {df} is the yield column\\n as it contains the text \\\"{y_col}\\\". It is located at location {loc}')\n",
    "        loc += 1\n",
    "\n",
    "    yield_cols_found = list(loc_yield_cols.keys())\n",
    "    target_cols=yield_cols_found[0]\n",
    "loc_yield_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding dates between heading and maturity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:25:15.702113Z",
     "start_time": "2021-10-02T22:25:15.688140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GrainYield',\n",
       " 'Name',\n",
       " 'CodeName',\n",
       " 'Pedigree',\n",
       " 'Line',\n",
       " 'Heading_Date',\n",
       " 'Maturity_Date',\n",
       " 'Days2Heading',\n",
       " 'Days2Maturity',\n",
       " 'Lodging']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yield_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-02T22:25:15.718136Z",
     "start_time": "2021-10-02T22:25:15.704100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masbasis_2019_all\n",
      "Masbasis_2020_all_lodg\n",
      "Robot_2020_all\n",
      "Staur_2019_all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Graminor_2019_all',\n",
       " 'Graminor_2020_all',\n",
       " 'Masbasis_2019_all',\n",
       " 'Masbasis_2020_all_lodg',\n",
       " 'Robot_2020_all',\n",
       " 'Staur_2019_all',\n",
       " 'Staur_2020_all_lodg']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for df in all_df:\n",
    "    temp_df = locals()[df]\n",
    "    if 'Days2Maturity' in temp_df.columns:\n",
    "        print(df)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring the important dates for each field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T01:32:11.010603Z",
     "start_time": "2021-10-03T01:32:10.990636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dates listed in dict in order; sowing, heading, maturity\n",
    "# The order of fields must be the same as in all_df list\n",
    "# sowing_dict = {\n",
    "#     'Graminor_2019': ['240419', 'XX', 'XX'],\n",
    "#     'Graminor_2020': ['150420', 'XX', 'XX'],\n",
    "#     'Masbasis_2019': ['190519', 'XX', 'XX'],\n",
    "#     'Masbasis_2020': ['150520', 'XX', 'XX'],\n",
    "#     'Robot_2020': ['200420', '170620', '310720'],\n",
    "#     'Staur_2019': ['040619', 'XX', 'XX'],\n",
    "#     'Staur_2020': ['210420', 'XX', 'XX'],\n",
    "# }\n",
    "\n",
    "sowing_dict = {\n",
    "    'Graminor_2019': '240419',\n",
    "    'Graminor_2020': '150420',\n",
    "    'Masbasis_2019': '190519',\n",
    "    'Masbasis_2020': '150520',\n",
    "    'Robot_2020': '200420',\n",
    "    'Staur_2019': '040619',\n",
    "    'Staur_2020': '210420',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the df based on imp_dates dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T01:24:56.080292Z",
     "start_time": "2021-10-03T01:24:56.066308Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Creating a list to add the names of new filtered df\n",
    "# all_df_dates_filtered = {}\n",
    "\n",
    "# for field, key in zip_longest(all_df, imp_dates):\n",
    "#     # Checking if the field df and key in the dict are for the same field\n",
    "#     print(field, key)\n",
    "#     assert field.split('_')[0] == key.split('_')[0]\n",
    "    \n",
    "#     # Getting dates from imp_dates dict\n",
    "#     sowing, maturity, heading = imp_dates[key]\n",
    "   \n",
    "#     sowing_date = datetime.datetime.strptime(sowing, '%d%m%y').date()\n",
    "#     heading_date = datetime.datetime.strptime(maturity, '%d%m%y').date()\n",
    "#     maturity_date =datetime.datetime.strptime(heading, '%d%m%y').date()\n",
    "    \n",
    "#     # Iterating through all base indices column names\n",
    "#     for col in general_col_names[1:]:\n",
    "\n",
    "#         cols_current = [x for x in locals()[field].columns if col+'_' in x]\n",
    "#         dates = [x.split('_')[1] for x in cols_current]\n",
    "#         date_fmt = [datetime.datetime.strptime(x, '%d%m%y').date() for x in dates]\n",
    "#         # Listing the dates in between(and including) heading and maturity dates\n",
    "#         in_between_dates = [x.strftime(\"%d%m%y\") for x in date_fmt\\\n",
    "#                             if x >= heading_date and x <= maturity_date]\n",
    "                        \n",
    "#         dates_not_usable = [x.strftime(\"%d%m%y\") for x in date_fmt\\\n",
    "#                              if not x.strftime(\"%d%m%y\") in in_between_dates]\n",
    "\n",
    "#     # Filter the datasets with date between heading and maturity\n",
    "#     select_cols = [x for x in locals()[field].columns if x[-6:] not in dates_not_usable]\n",
    "#     temp_df = locals()[field][select_cols]\n",
    "\n",
    "#     # Adding the names of new df into all_df_dates_filtered list\n",
    "#     filtered_df = key+'_dates_filtered'\n",
    "#     all_df_dates_filtered[filtered_df] = imp_dates[key]\n",
    "#     locals()[filtered_df] = temp_df\n",
    "# all_df_dates_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering df which have Days2Maturity and Days2Heading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T01:36:41.864446Z",
     "start_time": "2021-10-03T01:36:41.854946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masbasis_2019_all\n",
      "Masbasis_2020_all_lodg\n",
      "Robot_2020_all\n",
      "Staur_2019_all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Masbasis_2019_all': '190519',\n",
       " 'Masbasis_2020_all_lodg': '150520',\n",
       " 'Robot_2020_all': '200420',\n",
       " 'Staur_2019_all': '040619'}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the dataset had Days 2 heading and days to maturity columns then create the\n",
    "# following dictionary with the respective sowing dates of each field as value\n",
    "all_df_dates_filtered = {}\n",
    "\n",
    "for df in all_df:\n",
    "    temp_df = locals()[df]\n",
    "    field_temp = df.split('_')[0]+'_'+df.split('_')[1]\n",
    "    if 'Days2Heading' in temp_df.columns and 'Days2Maturity' in temp_df.columns:\n",
    "        print(df)\n",
    "        all_df_dates_filtered[df] = sowing_dict[field_temp]\n",
    "all_df_dates_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T01:36:42.480984Z",
     "start_time": "2021-10-03T01:36:42.472020Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import simps\n",
    "from numpy import trapz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating df with Plot_ID and Grain_Yield only\n",
    "## Calculating AUC and creating new df with calculated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T01:36:43.413327Z",
     "start_time": "2021-10-03T01:36:43.393380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Plot_ID',\n",
       " 'GrainYield',\n",
       " 'Name',\n",
       " 'Pedigree',\n",
       " 'Lodging',\n",
       " 'NDVI_200620',\n",
       " 'MTCI_200620',\n",
       " 'DVI_200620',\n",
       " 'GDVI_200620',\n",
       " 'MTCI_CI_200620',\n",
       " 'EXG_200620',\n",
       " 'EXGR_200620',\n",
       " 'RDVI_200620',\n",
       " 'TDVI_200620',\n",
       " 'GNDVI_200620',\n",
       " 'NDRE_200620',\n",
       " 'SCCI_200620',\n",
       " 'EVI_200620',\n",
       " 'TVI_200620',\n",
       " 'VARI_200620',\n",
       " 'GARI_200620',\n",
       " 'GCI_200620',\n",
       " 'GLI_200620',\n",
       " 'NLI_200620',\n",
       " 'MNLI_200620',\n",
       " 'SAVI_200620',\n",
       " 'GSAVI_200620',\n",
       " 'OSAVI_200620',\n",
       " 'GOSAVI_200620',\n",
       " 'MSAVI2_200620',\n",
       " 'MSR_200620',\n",
       " 'GRVI_200620',\n",
       " 'WDRVI_200620',\n",
       " 'SR_200620',\n",
       " 'NDVI_250620',\n",
       " 'MTCI_250620',\n",
       " 'DVI_250620',\n",
       " 'GDVI_250620',\n",
       " 'MTCI_CI_250620',\n",
       " 'EXG_250620',\n",
       " 'EXGR_250620',\n",
       " 'RDVI_250620',\n",
       " 'TDVI_250620',\n",
       " 'GNDVI_250620',\n",
       " 'NDRE_250620',\n",
       " 'SCCI_250620',\n",
       " 'EVI_250620',\n",
       " 'TVI_250620',\n",
       " 'VARI_250620',\n",
       " 'GARI_250620',\n",
       " 'GCI_250620',\n",
       " 'GLI_250620',\n",
       " 'NLI_250620',\n",
       " 'MNLI_250620',\n",
       " 'SAVI_250620',\n",
       " 'GSAVI_250620',\n",
       " 'OSAVI_250620',\n",
       " 'GOSAVI_250620',\n",
       " 'MSAVI2_250620',\n",
       " 'MSR_250620',\n",
       " 'GRVI_250620',\n",
       " 'WDRVI_250620',\n",
       " 'SR_250620',\n",
       " 'NDVI_090720',\n",
       " 'MTCI_090720',\n",
       " 'DVI_090720',\n",
       " 'GDVI_090720',\n",
       " 'MTCI_CI_090720',\n",
       " 'EXG_090720',\n",
       " 'EXGR_090720',\n",
       " 'RDVI_090720',\n",
       " 'TDVI_090720',\n",
       " 'GNDVI_090720',\n",
       " 'NDRE_090720',\n",
       " 'SCCI_090720',\n",
       " 'EVI_090720',\n",
       " 'TVI_090720',\n",
       " 'VARI_090720',\n",
       " 'GARI_090720',\n",
       " 'GCI_090720',\n",
       " 'GLI_090720',\n",
       " 'NLI_090720',\n",
       " 'MNLI_090720',\n",
       " 'SAVI_090720',\n",
       " 'GSAVI_090720',\n",
       " 'OSAVI_090720',\n",
       " 'GOSAVI_090720',\n",
       " 'MSAVI2_090720',\n",
       " 'MSR_090720',\n",
       " 'GRVI_090720',\n",
       " 'WDRVI_090720',\n",
       " 'SR_090720',\n",
       " 'NDVI_160720',\n",
       " 'MTCI_160720',\n",
       " 'DVI_160720',\n",
       " 'GDVI_160720',\n",
       " 'MTCI_CI_160720',\n",
       " 'EXG_160720',\n",
       " 'EXGR_160720',\n",
       " 'RDVI_160720',\n",
       " 'TDVI_160720',\n",
       " 'GNDVI_160720',\n",
       " 'NDRE_160720',\n",
       " 'SCCI_160720',\n",
       " 'EVI_160720',\n",
       " 'TVI_160720',\n",
       " 'VARI_160720',\n",
       " 'GARI_160720',\n",
       " 'GCI_160720',\n",
       " 'GLI_160720',\n",
       " 'NLI_160720',\n",
       " 'MNLI_160720',\n",
       " 'SAVI_160720',\n",
       " 'GSAVI_160720',\n",
       " 'OSAVI_160720',\n",
       " 'GOSAVI_160720',\n",
       " 'MSAVI2_160720',\n",
       " 'MSR_160720',\n",
       " 'GRVI_160720',\n",
       " 'WDRVI_160720',\n",
       " 'SR_160720',\n",
       " 'NDVI_240720',\n",
       " 'MTCI_240720',\n",
       " 'DVI_240720',\n",
       " 'GDVI_240720',\n",
       " 'MTCI_CI_240720',\n",
       " 'EXG_240720',\n",
       " 'EXGR_240720',\n",
       " 'RDVI_240720',\n",
       " 'TDVI_240720',\n",
       " 'GNDVI_240720',\n",
       " 'NDRE_240720',\n",
       " 'SCCI_240720',\n",
       " 'EVI_240720',\n",
       " 'TVI_240720',\n",
       " 'VARI_240720',\n",
       " 'GARI_240720',\n",
       " 'GCI_240720',\n",
       " 'GLI_240720',\n",
       " 'NLI_240720',\n",
       " 'MNLI_240720',\n",
       " 'SAVI_240720',\n",
       " 'GSAVI_240720',\n",
       " 'OSAVI_240720',\n",
       " 'GOSAVI_240720',\n",
       " 'MSAVI2_240720',\n",
       " 'MSR_240720',\n",
       " 'GRVI_240720',\n",
       " 'WDRVI_240720',\n",
       " 'SR_240720',\n",
       " 'NDVI_310720',\n",
       " 'MTCI_310720',\n",
       " 'DVI_310720',\n",
       " 'GDVI_310720',\n",
       " 'MTCI_CI_310720',\n",
       " 'EXG_310720',\n",
       " 'EXGR_310720',\n",
       " 'RDVI_310720',\n",
       " 'TDVI_310720',\n",
       " 'GNDVI_310720',\n",
       " 'NDRE_310720',\n",
       " 'SCCI_310720',\n",
       " 'EVI_310720',\n",
       " 'TVI_310720',\n",
       " 'VARI_310720',\n",
       " 'GARI_310720',\n",
       " 'GCI_310720',\n",
       " 'GLI_310720',\n",
       " 'NLI_310720',\n",
       " 'MNLI_310720',\n",
       " 'SAVI_310720',\n",
       " 'GSAVI_310720',\n",
       " 'OSAVI_310720',\n",
       " 'GOSAVI_310720',\n",
       " 'MSAVI2_310720',\n",
       " 'MSR_310720',\n",
       " 'GRVI_310720',\n",
       " 'WDRVI_310720',\n",
       " 'SR_310720']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = locals()[df].columns\n",
    "non_indices_cols = [x for x in cols if x.split('_')[0] not in general_col_names[1:]]\n",
    "non_indices_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_df_all = []\n",
    "for df, sowing in all_df_dates_filtered.items():\n",
    "\n",
    "    # Creating a list of columns which are not present in \n",
    "    # the general_col_names list (except the first item,i.e. Plot_ID)\n",
    "    temp_df = locals()[df].copy()\n",
    "    cols = temp_df.columns\n",
    "    non_indices_cols = [x for x in cols if x.split('_')[0] not in general_col_names[1:]]\n",
    "\n",
    "#     # ORR\n",
    "#     non_indices_cols = id_cols_new+yield_cols_found\n",
    "#     non_indices_cols\n",
    "\n",
    "    df_auc = temp_df[non_indices_cols]\n",
    "#     display(df_auc.head())\n",
    "\n",
    "    # Calculating AUC and creating new df with calculated values\n",
    "    for col_name in general_col_names[1:]:\n",
    "        df_simp = []\n",
    "        temp_cols = [x for x in cols if col_name+'_' in x]\n",
    "        temp_dates = [datetime.datetime.strptime(date.split('_')[1], '%d%m%y').date() for date in temp_cols]\n",
    "\n",
    "        # Calculating the days from sowing,i.e. age of the crop in days\n",
    "        sowing_date = datetime.datetime.strptime(sowing, '%d%m%y').date() \n",
    "        \n",
    "\n",
    "        for sample in range(temp_df.shape[0]):\n",
    "            # Number of days since sowing for each entry\n",
    "            days_sow = [(x-sowing_date).days for x in temp_dates]\n",
    "            # The respective value of the index in question \n",
    "            temp_entries= [temp_df[x][sample] for x in temp_cols]\n",
    "\n",
    "            #### DROPPING DATES OUTSIDE HEADING AND MATURITY DATES ####\n",
    "            # Days to heading for current sample \n",
    "            DH = temp_df[Days2Heading][sample]\n",
    "            # Days to maturity for current sample \n",
    "            DM = temp_df[Days2Maturity][sample]\n",
    "            \n",
    "            # Making sure that the maturity comes after heading\n",
    "            assert DM > DH\n",
    "            \n",
    "            heading_date = sowing_date + datetime.timedelta(days=DH)\n",
    "            maturity_date = sowing_date + datetime.timedelta(days=DM)\n",
    "            \n",
    "            # Replacing the respective values of items in temp_entries with np.nan which correspond \n",
    "            # to dates not in between heading and maturity for that specific sub-plot\n",
    "            temp_entries = [y if heading_date <= x <= maturity_date else np.nan for x,y in zip(temp_dates, temp_entries)]\n",
    "            \n",
    "            # Dropping missing(nan) values from the entries\n",
    "            temp_entries_dropna = [x for x in temp_entries if str(x) != 'nan']\n",
    "\n",
    "            # Checking if the number of items in temp_entries and days_sow is the same\n",
    "            # If not, i.e., there are missing values(nan) in temp_entries then drop the\n",
    "            # respective entries from days_sow list\n",
    "            if not len(temp_entries_dropna) == len(days_sow):\n",
    "                # Dictionary comprehension\n",
    "                # Creating dictionary(dict comprehension) where temp_entries are not nan\n",
    "                dict_dropna = {i: [temp_entries[i], days_sow[i]] for i in range(len(temp_entries))\\\n",
    "                       if not str(temp_entries[i]) == 'nan' }\n",
    "                \n",
    "                # Checking if the previously created temp_entries_dropna is the same as the new that will\n",
    "                # be created from dict_dropna (Unnecessary check but curious to check if any problems arise)\n",
    "                assert temp_entries_dropna == [dict_dropna[i][0] for i in dict_dropna.keys()]\n",
    "                \n",
    "                # Creating new temp_entries and days_sow after dropping nan and respective entries in days_sow\n",
    "                temp_entries_dropna = [dict_dropna[i][0] for i in dict_dropna.keys()]\n",
    "                days_sow = [dict_dropna[i][1] for i in dict_dropna.keys()]\n",
    "\n",
    "#             Checking if the lists have the same number of entries\n",
    "            assert len(temp_entries_dropna) == len(days_sow)\n",
    "\n",
    "            df_simp.append(simps(temp_entries_dropna, days_sow))\n",
    "\n",
    "        # Insert the new column at the end, but before GrainYield\n",
    "        df_auc.insert(len(df_auc.columns)-1, col_name, df_simp)\n",
    "\n",
    "    # Adding the new name of the df to a list named simp_df_all\n",
    "    simp_df = df[:-15]+'_Simps'\n",
    "    simp_df_all.append(simp_df)\n",
    "    locals()[simp_df] = df_auc.copy()\n",
    "simp_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-03T00:46:03.996095Z",
     "start_time": "2021-10-03T00:46:03.969591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simp_df_all = []\n",
    "for df, imp_dates in all_df_dates_filtered.items():\n",
    "\n",
    "    # Creating a list of columns which are not present in \n",
    "    # the general_col_names list (except the first item,i.e. Plot_ID)\n",
    "    temp_df = locals()[df].copy()\n",
    "    cols = temp_df.columns\n",
    "    non_indices_cols = [x for x in cols if x.split('_')[0] not in general_col_names[1:]]\n",
    "\n",
    "#     # ORR\n",
    "#     non_indices_cols = id_cols_new+yield_cols_found\n",
    "#     non_indices_cols\n",
    "\n",
    "    df_auc = temp_df[non_indices_cols]\n",
    "#     display(df_auc.head())\n",
    "\n",
    "    # Calculating AUC and creating new df with calculated values\n",
    "    for col_name in general_col_names[1:]:\n",
    "        df_simp = []\n",
    "        temp_cols = [x for x in cols if col_name+'_' in x]\n",
    "        temp_dates = [datetime.datetime.strptime(date.split('_')[1], '%d%m%y').date() for date in temp_cols]\n",
    "\n",
    "        # Calculating the days from sowing,i.e. age of the crop in days\n",
    "        sowing_date = datetime.datetime.strptime(imp_dates[0], '%d%m%y').date() \n",
    "        \n",
    "\n",
    "        for sample in range(temp_df.shape[0]):\n",
    "            # Number of days since sowing for each entry\n",
    "            days_sow = [(x-sowing_date).days for x in temp_dates]\n",
    "            # The respective value of the index in question \n",
    "            temp_entries= [temp_df[x][sample] for x in temp_cols]\n",
    "            # Dropping missing(nan) values from the entries\n",
    "            temp_entries_dropna = [x for x in temp_entries if str(x) != 'nan']\n",
    "\n",
    "            # Checking if the number of items in temp_entries and days_sow is the same\n",
    "            # If not, i.e., there are missing values(nan) in temp_entries then drop the\n",
    "            # respective entries from days_sow list\n",
    "            if not len(temp_entries_dropna) == len(days_sow):\n",
    "                # Dictionary comprehension\n",
    "                # Creating dictionary where temp_entries are not nan\n",
    "                dict_dropna = {i: [temp_entries[i], days_sow[i]] for i in range(len(temp_entries))\\\n",
    "                       if not str(temp_entries[i]) == 'nan' }\n",
    "                \n",
    "                # Creating new temp_entries and days_sow after dropping nan and respective entries in days_sow\n",
    "                temp_entries_dropna = [dict_dropna[i][0] for i in dict_dropna.keys()]\n",
    "                days_sow = [dict_dropna[i][1] for i in dict_dropna.keys()]\n",
    "\n",
    "#             Checking if the lists have the same number of entries\n",
    "            assert len(temp_entries_dropna) == len(days_sow)\n",
    "\n",
    "            df_simp.append(simps(temp_entries_dropna, days_sow))\n",
    "\n",
    "        # Insert the new column at the end, but before GrainYield\n",
    "        df_auc.insert(len(df_auc.columns)-1, col_name, df_simp)\n",
    "\n",
    "    # Adding the new name of the df to a list named simp_df_all\n",
    "    simp_df = df[:-15]+'_Simps'\n",
    "    simp_df_all.append(simp_df)\n",
    "    locals()[simp_df] = df_auc.copy()\n",
    "simp_df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp: Exporting data to be used for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = export_path+'Temp_Data/'\n",
    "os.makedirs(temp_data, exist_ok=True)\n",
    "# for df in simp_df_all:\n",
    "#     locals()[df].to_csv(temp_data+df+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Genomics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Genomics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Yield data with line information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:56:14.818870Z",
     "start_time": "2021-09-08T17:56:14.815434Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vollebekk 2019: Graminor_2019_x_19TvPhenores_x_Vollebekk_res\n",
    "# Masbasis 2020: Masbasis_x_20BMLGI1_2020_tm_x_data\n",
    "# Robot 2020: Robot_x_ROBOT_2020_x_raw\n",
    "# Masbasis 2019: Masbasis_2019_x_Field_data_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:56:14.833002Z",
     "start_time": "2021-09-08T17:56:14.824218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Graminor 2019': ['Graminor_2019_x_19TvPhenores_x_Vollebekk_res',\n",
      "                   '\\\\MegaSync\\\\NMBU\\\\Master '\n",
      "                   'Thesis\\\\Data\\\\Feb2021\\\\Graminor_2019\\\\19TvPhenores.xlsx'],\n",
      " 'Masbasis 2019': ['Masbasis_2019_x_Field_data_2019',\n",
      "                   '\\\\MegaSync\\\\NMBU\\\\Master '\n",
      "                   'Thesis\\\\Data\\\\Feb2021\\\\Masbasis_2019\\\\Field_data_2019.xlsx'],\n",
      " 'Masbasis 2020': ['Masbasis_x_20BMLGI1_2020_tm_x_data',\n",
      "                   '\\\\MegaSync\\\\NMBU\\\\Master '\n",
      "                   'Thesis\\\\Data\\\\Feb2021\\\\Vollebekke-total_2020\\\\Masbasis\\\\20BMLGI1_2020_tm.xlsx'],\n",
      " 'Robot 2020': ['Robot_x_ROBOT_2020_x_raw',\n",
      "                '\\\\MegaSync\\\\NMBU\\\\Master '\n",
      "                'Thesis\\\\Data\\\\Feb2021\\\\Vollebekke-total_2020\\\\Robot\\\\ROBOT_2020.xlsx'],\n",
      " 'Staur 2019': ['Graminor_2019_x_19TvPhenores_x_Staur_res',\n",
      "                '\\\\MegaSync\\\\NMBU\\\\Master '\n",
      "                'Thesis\\\\Data\\\\Feb2021\\\\Graminor_2019\\\\19TvPhenores.xlsx']}\n"
     ]
    }
   ],
   "source": [
    "a_file = open(main_path+'yield_df.json', \"r\")\n",
    "output_str = a_file.read()\n",
    "# The file is imported as string\n",
    "\n",
    "# Converting it to dictionary\n",
    "output_dict = json.loads(output_str)\n",
    "a_file.close()\n",
    "\n",
    "pprint(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking number of unique cultivars in the field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:56:14.839834Z",
     "start_time": "2021-09-08T17:56:14.835929Z"
    }
   },
   "outputs": [],
   "source": [
    "# plots_data = pd.read_excel(files_with_address[0],engine='openpyxl')\n",
    "# # Pandas converts 'NA' string to NaN. Need to change those to \n",
    "# # some string to get a count as NaNs are not counted as unique values\n",
    "\n",
    "# plots_data.Name.fillna('-', inplace=True)\n",
    "# plots_data.CodeName.fillna('-', inplace=True)\n",
    "\n",
    "# # Creating a new column as multiple plots were named 'NA' but the \n",
    "# # CodeName was different for each one of them\n",
    "# plots_data['NameCode'] = plots_data.Name+plots_data.CodeName\n",
    "\n",
    "# plots_data\n",
    "# len(plots_data.NameCode.unique())\n",
    "# plots_data.NameCode.value_counts()\n",
    "# # plots_data.NameCode.value_counts().sum()\n",
    "# # plots_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo: Dropping NAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding NAN values\n",
    "### ToDo: Test: Raise error if missing values found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:16:55.599254Z",
     "start_time": "2021-09-08T17:16:55.592423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing value found in any dataframe\n"
     ]
    }
   ],
   "source": [
    "# Finding number of missing values in each dataframe\n",
    "df_with_nan = []\n",
    "missing_values = False\n",
    "for df in all_df:\n",
    "    if locals()[df].isna().sum().sum() > 0:\n",
    "        print(f'Total missing values in {df} are {locals()[df].isna().sum().sum()}')\n",
    "        missing_values = True\n",
    "        df_with_nan.append(df)\n",
    "#     if len(df_with_nan) > 0:\n",
    "#         raise ValueError\n",
    "if not missing_values:\n",
    "    print('No missing value found in any dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:16:55.623191Z",
     "start_time": "2021-09-08T17:16:55.601230Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Graminor_2019_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-92e745b63b43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mGraminor_2019_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Graminor_2019_all' is not defined"
     ]
    }
   ],
   "source": [
    "Graminor_2019_all.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:16:55.632951Z",
     "start_time": "2021-09-08T17:16:55.626119Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:16:55.640759Z",
     "start_time": "2021-09-08T17:16:55.635878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finding which column has NAN values\n",
    "for df in df_with_nan:\n",
    "    print(f'{df}:\\n {locals()[df].shape[1]-locals()[df].dropna(axis=1).shape[1]} columns or {locals()[df].shape[0]-locals()[df].dropna().shape[0]} rows to be dropped,')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo: Automate: Drop rows with missing values in df_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:16:55.936906Z",
     "start_time": "2021-09-08T17:16:55.923195Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Graminor_eastwest_020719_NIR_half_missing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-bd632eb495de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{Graminor_eastwest_020719_NIR_half_missing.shape} Before dropping'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Graminor_eastwest_020719_NIR_half_missing.dropna(inplace=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{Graminor_eastwest_020719_NIR_half_missing.shape} After dropping'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Graminor_eastwest_020719_NIR_half_missing' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'{Graminor_eastwest_020719_NIR_half_missing.shape} Before dropping')\n",
    "# Graminor_eastwest_020719_NIR_half_missing.dropna(inplace=True)\n",
    "print(f'{Graminor_eastwest_020719_NIR_half_missing.shape} After dropping')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo: Droppping df with Nan from the all_df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:16:56.595993Z",
     "start_time": "2021-09-08T17:16:56.591114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in all_df is 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of items in all_df is {len(all_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:16:56.602824Z",
     "start_time": "2021-09-08T17:16:56.598923Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for df in df_with_nan:\n",
    "#     all_df.remove(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ToDo: Update field_year_dict and sorted_field_year_dict after dropping the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:16:56.943290Z",
     "start_time": "2021-09-08T17:16:56.938408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in all_df now is 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of items in all_df now is {len(all_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo:  \n",
    "see the distribution of data if it is normal  \n",
    "else make transpose to make it normal  \n",
    "dist in Gausion function   \n",
    "in each field  \n",
    "what if the data is normal dist?  \n",
    "the use some transpose to box pox   \n",
    "try diff funct to see which one iis able to make data normal  \n",
    "make heat map of whole if not normal  \n",
    "see which parts are not normal and exculde them  \n",
    "ls_means in R to make the normalisation/transpose  \n",
    "pearson corr bw yield and indices for diff dates  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:16:57.911154Z",
     "start_time": "2021-09-08T17:16:57.898466Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-1ae58ac70b7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_labels' is not defined"
     ]
    }
   ],
   "source": [
    "x_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yeo-Johnson Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:16:59.631522Z",
     "start_time": "2021-09-08T17:16:58.258599Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'field_year_dict_yield' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-bf2287054a1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_agg_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcol_for_plotting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield_year_dict_yield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'field_year_dict_yield' is not defined"
     ]
    }
   ],
   "source": [
    "col_for_plotting = ['Blue', 'Green', 'Red', 'RedEdge', 'NIR', 'NDVI', 'MTCI', 'EVI']\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer, normalize, StandardScaler\n",
    "data_agg_list = ['_median_indices']\n",
    "\n",
    "# col_for_plotting = ['Blue']\n",
    "# col_for_plotting = ['Green']\n",
    "# col_for_plotting = ['Red']\n",
    "\n",
    "for d_type in data_agg_list:\n",
    "    for col in col_for_plotting:\n",
    "        fields = len(field_year_dict_yield.keys())\n",
    "        rows = math.ceil(fields/2)\n",
    "        \n",
    "        fig, ax = plt.subplots(rows,2, figsize=(15,10))\n",
    "        plots = ax.flatten()\n",
    "        n = 0\n",
    "        # TODO: Fix the x ticks\n",
    "        \n",
    "\n",
    "        for field_sample, dates in sorted_field_year_dict_yield.items():\n",
    "            x_labels = []\n",
    "            # Adding required data to a temp dataframe\n",
    "            temp_df = pd.DataFrame()\n",
    "            for date in dates:\n",
    "                date_str = date.strftime('%d%m%y')\n",
    "                field_df = field_sample[:-5]+'_'+date_str+d_type\n",
    "                temp_df[date] = locals()[field_df][col]\n",
    "                x_label = date.strftime('%d-%m-%y')+':'+str(len(locals()[field_df][col]))\n",
    "                \n",
    "                x_labels.append(x_label)\n",
    "                x_labels= list(set(x_labels))\n",
    "            # Transform the df\n",
    "#             pt = PowerTransformer(method='box-cox', standardize=False)\n",
    "            pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "\n",
    "            temp_arr = pt.fit_transform(temp_df)\n",
    "            temp_df = pd.DataFrame(temp_arr)\n",
    "            \n",
    "            # Adding field plot to the subplots\n",
    "            num_of_fields = len(field_year_dict_yield.keys())\n",
    "            \n",
    "            text = \"Grain Yield\"\n",
    "            ax_n = plots[n]\n",
    "            \n",
    "            temp_df.boxplot(ax=ax_n)\n",
    "            ax_n.set_xticklabels(x_labels, rotation=-35)\n",
    "            ax_n.set_title(field_sample+'_'+col+d_type[:-5]+'_yeo-johnson')\n",
    "            \n",
    "#             # Printing the grain yield in plot of the fiels_sample for reference\n",
    "#             ax_n.text(0.85, 1.05, text, ha='center', va='top', weight='bold', color='blue', transform=ax_n.transAxes)\n",
    "            n+=1\n",
    "        plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-Cox Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:16:59.656899Z",
     "start_time": "2021-09-08T17:16:59.633474Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'field_year_dict_yield' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-8b39c1ee47a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_agg_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcol_for_plotting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield_year_dict_yield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'field_year_dict_yield' is not defined"
     ]
    }
   ],
   "source": [
    "col_for_plotting = ['Blue', 'Green', 'Red', 'RedEdge', 'NIR', 'NDVI', 'MTCI', 'EVI']\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer, normalize, StandardScaler\n",
    "data_agg_list = ['_median_indices']\n",
    "\n",
    "# col_for_plotting = ['Blue']\n",
    "# col_for_plotting = ['Green']\n",
    "# col_for_plotting = ['Red']\n",
    "\n",
    "for d_type in data_agg_list:\n",
    "    for col in col_for_plotting:\n",
    "        fields = len(field_year_dict_yield.keys())\n",
    "        rows = math.ceil(fields/2)\n",
    "        \n",
    "        fig, ax = plt.subplots(rows,2, figsize=(15,10))\n",
    "        plots = ax.flatten()\n",
    "        n = 0\n",
    "        # TODO: Fix the x ticks\n",
    "        for field_sample, dates in sorted_field_year_dict_yield.items():\n",
    "            \n",
    "            # Adding required data to a temp dataframe\n",
    "            temp_df = pd.DataFrame()\n",
    "            for date in dates:\n",
    "                date_str = date.strftime('%d%m%y')\n",
    "                field_df = field_sample[:-5]+'_'+date_str+d_type\n",
    "                temp_df[date] = locals()[field_df][col]\n",
    "            x_labels = temp_df.columns.tolist()\n",
    "\n",
    "            # Transform the df\n",
    "#             pt = PowerTransformer(method='box-cox', standardize=False)\n",
    "            pt = PowerTransformer(method='box-cox', standardize=False)\n",
    "\n",
    "            # Taking absolute values of the dataframe(avoiding negative values)\n",
    "            temp_arr = pt.fit_transform(temp_df.abs())\n",
    "            temp_df = pd.DataFrame(temp_arr)\n",
    "            \n",
    "            # Adding field plot to the subplots\n",
    "            num_of_fields = len(field_year_dict_yield.keys())\n",
    "            \n",
    "            text = \"Grain Yield\"\n",
    "            ax_n = plots[n]\n",
    "\n",
    "            temp_df.boxplot(ax=ax_n)\n",
    "            ax_n.set_xticklabels(x_labels, rotation=90)\n",
    "            ax_n.set_title(field_sample+'_'+col+d_type[:-5]+'_box-cox')\n",
    "            \n",
    "#             # Printing the grain yield in plot of the fiels_sample for reference\n",
    "#             ax_n.text(0.85, 1.05, text, ha='center', va='top', weight='bold', color='blue', transform=ax_n.transAxes)\n",
    "            n+=1\n",
    "        plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo: Identify Dates and index with problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecxclude the problematic data/dates\n",
    "or\n",
    "### Take average values where the problematic data is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take average of data for date 20200708 and 20200624  \n",
    "Masbasis  \n",
    "Cleanup  \n",
    "Remove dates which have drop  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo: Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find AUC for all dates of one field\n",
    "See if it covers tha gaps under the dates,i.e.\n",
    "\n",
    "Since data points are different  \n",
    "Flying time is different  \n",
    "Cover the gaps between the dates  \n",
    "\n",
    "Since the data collection is not uniform throughout the year so AUC will give a single value instead of multiple values for one field year which will be representative of all the dates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Use Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:17:01.226553Z",
     "start_time": "2021-09-08T17:17:01.218744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:17:01.241194Z",
     "start_time": "2021-09-08T17:17:01.235336Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "from scipy.integrate import simps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:17:01.254854Z",
     "start_time": "2021-09-08T17:17:01.248025Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import simpson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:17:01.280233Z",
     "start_time": "2021-09-08T17:17:01.274374Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 10)\n",
    "y = np.arange(0, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:17:01.306584Z",
     "start_time": "2021-09-08T17:17:01.296829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# integrate.simpson(y, x)\n",
    "integrate.simps(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:17:01.333910Z",
     "start_time": "2021-09-08T17:17:01.323176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   8,  27,  64, 125, 216, 343, 512, 729], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.power(x, 3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:17:01.356360Z",
     "start_time": "2021-09-08T17:17:01.347576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1642.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrate.simpson(y, x)\n",
    "# integrate.simps(y, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:17:01.372949Z",
     "start_time": "2021-09-08T17:17:01.364165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1640.25"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrate.quad(lambda x: x**3, 0, 9)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:17:01.392469Z",
     "start_time": "2021-09-08T17:17:01.380762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1644.5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrate.simpson(y, x, even='first')\n",
    "# integrate.simps(y, x, even='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:17:01.969977Z",
     "start_time": "2021-09-08T17:17:01.954361Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-5c4058e7910f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# plot: Plot ID\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# x: Number of days after sowing or actual date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# y: Value of the index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data\n",
    "# plot: Plot ID\n",
    "# x: Number of days after sowing or actual date\n",
    "# y: Value of the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:17:01.991449Z",
     "start_time": "2021-09-08T17:17:01.973882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8500000000000001\n",
      "1.5\n",
      "1.95\n"
     ]
    }
   ],
   "source": [
    "# x: Days from sowing to data collection\n",
    "# May 5 2019 Masbasis and Graminor\n",
    "# Robot: \n",
    "\n",
    "data={'plot':['1','1','2','2','3','3'],'x':['5','6','7','8','9','10'],'y':['0.9','0.8','0.7','0.6','0.5','0.4'] }\n",
    "\n",
    "ACC=[]\n",
    "A=pd.DataFrame(data, columns=['plot','x','y'])\n",
    "AA=0\n",
    "\n",
    "for item in range(len(A)-1):\n",
    "    if A['plot'][item]== A['plot'][item+1]:\n",
    "        Ans=(float((A['y'][item]))+float((A['y'][item+1])))*((float((A['x'][item+1]))-float((A['x'][item]))))/2\n",
    "        AA+=Ans\n",
    "        print(AA)\n",
    "        ACC.append(AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T17:17:02.434915Z",
     "start_time": "2021-09-08T17:17:02.414078Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-508f0f07663b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Plot'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mACC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mNumbers_final\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Data' is not defined"
     ]
    }
   ],
   "source": [
    "df1=Data.set_index(['Plot'])\n",
    "ACC=[]\n",
    "\n",
    "for item in Numbers_final:\n",
    "    df2=df1[df1.index==item]\n",
    "    df2=df2.filter(['Blue', 'Green', 'Red', 'RedEdge', 'NIR','NDVI', 'MTCI', 'EVI', 'DVI', 'RVI', 'VARI', 'EXG', 'EXGR', 'GLI', 'GNDVI', 'GVI','Time','timepoint'], axis=1)\n",
    "    df2=df2.sort_values(by='timepoint')\n",
    "    df3=df2.reset_index()\n",
    "\n",
    "AA=0\n",
    "for j in range(0,3):\n",
    "    Ans=(float((df3['GVI'][j]))+float((df3['GVI'][j+1])))*((float((df3['timepoint'][j+1]))-float((df3['timepoint'][j]))))/2\n",
    "    AA+=Ans\n",
    "\n",
    "    print(AA)\n",
    "    ACC.append(AA)\n",
    "\n",
    "\n",
    "\n",
    "DA=pd.DataFrame(ACC)\n",
    "DD=pd.DataFrame(Numbers_final)\n",
    "DDA=pd.concat([DD, DA], axis=1)\n",
    "DDA.to_excel('Staur_Accumulative_GVI_2019.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series data vs the AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo: Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Make model for one year at a time and try to predict yield of another field  \n",
    "\n",
    "TODO: Train on Masbasis 2019 an 2020  \n",
    "Test on Staur  \n",
    "\n",
    "Use data until august for yield prediction since it is most relavant  \n",
    "Use all data for predicting date to maturity  \n",
    "\n",
    "Data Collection:  \n",
    "Data collection usually starts after heading  \n",
    "2019 has the data before hading as well. To use that, dont use dates before heading  \n",
    "\n",
    "NDVI is resistant to shadows  \n",
    "\n",
    "DAT390 Report: Do the report with Robot Data only  \n",
    "\n",
    "TODO: Use AUC for each index for prediction  \n",
    "\n",
    "TODO:   \n",
    "Time series data vs the AUC  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "391.442px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
