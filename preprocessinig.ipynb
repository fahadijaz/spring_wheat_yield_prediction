{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 520 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To display df nicely in loops\n",
    "from IPython.display import display \n",
    "# display(df1.head()) \n",
    "# display(df2.head())\n",
    "\n",
    "# Display rows and columns Pandas\n",
    "pd.options.display.max_columns = 100\n",
    "pd.set_option('display.max_rows',105)\n",
    "# columns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 998 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\fahad\\\\Documents\\\\GitHub\\\\Deep-Learning-for-Wheat-Yield-Prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Prints the current workinig directory\n",
    "os.getcwd()\n",
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 997 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Graminor_2019', 'Masbasis_2019', 'Staur_2019', 'Vollebekke-total_2020']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path = r'C:\\Users\\fahad\\Documents\\Master Thesis\\Phenotyping\\Data\\Feb2021'\n",
    "# path = path.replace(\"\\\\\", \"/\")+'/'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating list of all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get the list of all files in directory tree at given path\n",
    "\n",
    "files_with_address = []\n",
    "files_list = []\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "    files_with_address += [os.path.join(dirpath, file) for file in filenames]\n",
    "    files_list.extend(filenames)\n",
    "    \n",
    "# len(files_with_address)\n",
    "# files_with_address\n",
    "# files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "### Check for duplicate filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files are : 75\n",
      "Number of unique file names are: 74\n",
      "There is/are 1 duplicate file name/names.\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Total number of files are :', len(files_list))\n",
    "\n",
    "print('Number of unique file names are:', len(set(files_list)))\n",
    "\n",
    "print('There is/are', len(files_list)- len(set(files_list)),'duplicate file name/names.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the files with their paths from the data directory in a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.xlsx'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "file_w_address_test = files_with_address[1]\n",
    "\n",
    "# Get filename from path\n",
    "os.path.basename(file_w_address_test)\n",
    "\n",
    "# Get directory path name from path\n",
    "os.path.dirname(file_w_address_test)\n",
    "\n",
    "# Get directory name from path\n",
    "os.path.basename(os.path.dirname(file_w_address_test))\n",
    "\n",
    "# File name without extension\n",
    "os.path.splitext(os.path.basename(file_w_address_test))[0]\n",
    "\n",
    "# File Extension\n",
    "os.path.splitext(os.path.basename(file_w_address_test))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if there are multiple sheets in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following files have multiple sheets.\n",
      "4 19TvPhenores.xlsx in folder Graminor_2019\n",
      "4 19TvPhenores.xlsx in folder Staur_2019\n",
      "3 20BMLGI1_2020_tm.xlsx in folder Masbasis\n",
      "3 Masbasis_Mica_2020_all_dates_MEDIAN_DP.xlsx in folder Masbasis\n",
      "3 ROBOT_2020.xlsx in folder Robot\n",
      "Wall time: 6.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Print number of sheets in all files\n",
    "print('The following files have multiple sheets.')\n",
    "\n",
    "list_multi_sheet = []\n",
    "for file in files_with_address:\n",
    "    \n",
    "    xl_file = pd.ExcelFile(file,engine='openpyxl')\n",
    "    number_of_sheets = len(xl_file.sheet_names)\n",
    "    if number_of_sheets > 1:\n",
    "        print(number_of_sheets, os.path.basename(file), 'in folder', os.path.basename(os.path.dirname(file))\n",
    ")\n",
    "        list_multi_sheet.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Removing files with multiple sheets from the list\n",
    "\n",
    "for i in list_multi_sheet:\n",
    "    files_with_address.remove(i)\n",
    "len(files_with_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filding files without dates\n",
    "(with 2019 in name means they dont have date format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field_data_2019.xlsx in folder Masbasis_2019\n",
      "Masbasis_2019_ForDP_median.xlsx in folder Masbasis_2019\n",
      "Staur_Graminor_2019_median.xlsx in folder Staur_2019\n",
      "Staur_Masbasis_2019.xlsx in folder Staur_2019\n",
      "Wall time: 998 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "files_w_2019 = []\n",
    "for file in files_with_address:\n",
    "    file_name = os.path.basename(file)\n",
    "    if '2019' in file_name:\n",
    "        print(file_name, 'in folder', os.path.basename(os.path.dirname(file)))\n",
    "        files_w_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahad\\Documents\\Master Thesis\\Phenotyping\\Data\\Feb2021\\Masbasis_2019\\Field_data_2019.xlsx\n",
      "C:\\Users\\fahad\\Documents\\Master Thesis\\Phenotyping\\Data\\Feb2021\\Masbasis_2019\\Masbasis_2019_ForDP_median.xlsx\n",
      "C:\\Users\\fahad\\Documents\\Master Thesis\\Phenotyping\\Data\\Feb2021\\Staur_2019\\Staur_Graminor_2019_median.xlsx\n",
      "C:\\Users\\fahad\\Documents\\Master Thesis\\Phenotyping\\Data\\Feb2021\\Staur_2019\\Staur_Masbasis_2019.xlsx\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Removing from list\n",
    "for i in files_w_2019:\n",
    "    print(i)\n",
    "    files_with_address.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "len(files_with_address)\n",
    "# files_with_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data files to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graminor_070819_2\n",
      "Graminor_150819\n",
      "Graminor_east_050819\n",
      "Graminor_west_050819\n",
      "Graminor-250719\n",
      "Graminor_050719\n",
      "Graminor_150719\n",
      "Graminor_220719_corrected\n",
      "Graminor_east_020719\n",
      "Graminor_east_110719\n",
      "Graminor_west_020719\n",
      "Graminor_060619\n",
      "Graminor_110619\n",
      "Graminor_280619_corrected\n",
      "Graminor_east_250619_corrected\n",
      "Graminor_west_250619_correct\n",
      "Graminor_east_010720\n",
      "Graminor_east_040720\n",
      "Graminor_east_040820\n",
      "Graminor_east_070720_correct\n",
      "Graminor_east_090720\n",
      "Graminor_east_130720\n",
      "Graminor_east_140820\n",
      "Graminor_east_170720.cpg\n",
      "Graminor_east_180620\n",
      "Graminor_east_200720\n",
      "Graminor_east_240620_correct\n",
      "Graminor_east_300720.shp\n",
      "Graminor_Mica_west_240620\n",
      "Graminor_west_040720\n",
      "Graminor_west_040820\n",
      "Graminor_west_070720\n",
      "Graminor_west_070720_correct\n",
      "Graminor_west_090720\n",
      "Graminor_west_130720\n",
      "Graminor_west_140820\n",
      "Graminor_west_300720\n",
      "Masbasis_Mica_070820\n",
      "Masbasis_Mica_120820\n",
      "Masbasis_Mica_140820\n",
      "Masbasis_Mica_010720\n",
      "Masbasis_Mica_080720\n",
      "Masbasis_Mica_130720\n",
      "Masbasis_mica_170720\n",
      "Masbasis_Mica_220720\n",
      "Masbasis_Mica_300720\n",
      "Masbasis_mica_180620\n",
      "Masbasis_mica_240620\n",
      "Masbasis_Mica_260620\n",
      "Robot_Mica_010720\n",
      "Robot_Mica_040820\n",
      "Robot_Mica_070720\n",
      "Robot_Mica_070820\n",
      "Robot_Mica_120820\n",
      "Robot_Mica_130720\n",
      "Robot_Mica_140820\n",
      "Robot_Mica_160720\n",
      "Robot_mica_180620\n",
      "Robot_Mica_200720\n",
      "Robot_Mica_220720\n",
      "Robot_Mica_230620\n",
      "Robot_Mica_240620\n",
      "Robot_Mica_250620\n",
      "Robot_Mica_270720\n",
      "Robot_Mica_290620\n",
      "Robot_Mica_300720\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_df = []\n",
    "for data in files_with_address:\n",
    "    file_name = os.path.splitext(os.path.basename(data))[0]\n",
    "\n",
    "#     # Parent dir\n",
    "#     dir1_name = os.path.basename(os.path.dirname(data))\n",
    "#     # grand parent dir\n",
    "#     dir2_name = os.path.basename(os.path.dirname(os.path.dirname(data)))\n",
    "\n",
    "    # Replce all invalid characters in the name\n",
    "    file_name = file_name.replace(\" \", \"_\")\n",
    "    file_name = file_name.replace(\"(\", \"\")\n",
    "    df_name = file_name.replace(\")\", \"\")\n",
    "\n",
    "    all_df.append(df_name)\n",
    "\n",
    "    print(df_name)\n",
    "    locals()[df_name] = pd.read_excel(data, engine='openpyxl')\n",
    "# all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check which df have the data column heading we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "columns_req = []\n",
    "\n",
    "find_cols_mean = ['id', 'bluemea', 'greenme', 'redmea', 'reded', 'NIRmea', 'NDVImea', 'MTCImea']\n",
    "# 'RedEdge_2'\n",
    "\n",
    "find_cols_median = ['id', 'bluemed', 'greenmed', 'redmed', 'reded', 'NIRmed', 'NDVImed', 'MTCImed']\n",
    "# 'RedEdge_3'\n",
    "\n",
    "std_cols = ['plot', 'Blue', 'Green', 'Red', 'RedEdge', 'NIR', 'NDVI', 'MTCI', 'grain', 'yield']\n",
    "\n",
    "len(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing headings into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Finding max number of columns in all df\n",
    "\n",
    "len_columns = []\n",
    "for df in all_df:\n",
    "    cols_df = locals()[df].columns\n",
    "    len_columns.append(len(cols_df))\n",
    "max_cols_in_df = max(len_columns)\n",
    "\n",
    "# Now creating a empty df to collect all column headings\n",
    "columns_df = pd.DataFrame(data=range(0,max_cols_in_df), columns = ['ID'])\n",
    "columns_df.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "for df in all_df:\n",
    "    cols_df = locals()[df].columns\n",
    "    columns_df[df] = pd.Series(cols_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert columns_df to dictoionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.97 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Function to convert df to dict while dropping nan in each column separately\n",
    "\n",
    "def comp_dropna(df1):\n",
    "    return {k: v.dropna().to_dict() for k,v in df1.items()}\n",
    "\n",
    "columns_dict = comp_dropna(columns_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Max Colum heading with the text we are looking for in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot_ID column renamed in 66 of 66 found dataframes\n",
      "Blue column renamed in 61 of 61 found dataframes\n",
      "Green column renamed in 61 of 61 found dataframes\n",
      "Red column renamed in 62 of 62 found dataframes\n",
      "RedEdge column renamed in 62 of 62 found dataframes\n",
      "NIR column renamed in 60 of 60 found dataframes\n",
      "NDVI column renamed in 62 of 62 found dataframes\n",
      "MTCI column renamed in 61 of 61 found dataframes\n",
      "Wall time: 408 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:85: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Find columns with key from the following list\n",
    "# Rename the founc columns to the values against the keys\n",
    "find_cols = {\n",
    "    'id': 'Plot_ID',\n",
    "    'Blue': 'Blue',\n",
    "    'Gre': 'Green',\n",
    "    'Red': 'Red',\n",
    "    'RedE': 'RedEdge',\n",
    "    'NIR': 'NIR',\n",
    "    'NDVI': 'NDVI',\n",
    "    'MTCI': 'MTCI'\n",
    "}\n",
    "\n",
    "dtype_f = {0: '_Mean_Value', 1: '_Median_Value', 2: '_Std_Dev_Value'}\n",
    "num_std_dtype_colunms = len(dtype_f)\n",
    "\n",
    "list_ok_df_per_query = []\n",
    "list_problem_df_per_query = []\n",
    "\n",
    "for query, query_f in find_cols.items():\n",
    "    \n",
    "    # First, we loop through all df to find the max of number of relavant columns in any df\n",
    "    num_col_found_all = []\n",
    "    for df, cols_dict in columns_dict.items():\n",
    "\n",
    "        # List to save the relavant columns found in the df\n",
    "        cols_found = []\n",
    "        \n",
    "        # Loop through all column names\n",
    "        for key, item in cols_dict.items():\n",
    "            \n",
    "            # Check if required text is in colmn name,i.e. item\n",
    "            # When searching for Red, search for red but ignore RedEdge,i.e. RedE\n",
    "            if query.lower()=='red':\n",
    "                if item.lower().find(query.lower()) != -1 and item.lower().find('RedE'.lower()) == -1:\n",
    "                    cols_found.append(item)\n",
    "\n",
    "            # For the rest also checking we do not get 'GNDVI' columns. Don't neet them yet\n",
    "            elif item.lower().find(query.lower()) != -1 and item.lower().find('GNDVI'.lower()) == -1:\n",
    "                cols_found.append(item)\n",
    "\n",
    "        num_col_found_all.append(len(cols_found))\n",
    "    max_cols = max(num_col_found_all)\n",
    "#     print(query, max_cols)\n",
    "\n",
    "    # Define a df for saving the columns found if num of col is not num_std_dtype_colunms\n",
    "    temp_df_non_std_cols = 'col_df_non_std_cols_'+query\n",
    "    locals()[temp_df_non_std_cols] = pd.DataFrame(data=range(0,max_cols), columns = [query])\n",
    "    list_problem_df_per_query.append(temp_df_non_std_cols)\n",
    "    \n",
    "    # Define a df for saving the columns found if num of col is num_std_dtype_colunms\n",
    "    temp_df_std_cols= 'col_df_std_cols_'+query\n",
    "    locals()[temp_df_std_cols] = pd.DataFrame(data=range(0,num_std_dtype_colunms), columns = [query])\n",
    "    list_ok_df_per_query.append(temp_df_std_cols)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Now we again loop and append those columns to a relavant df for each query\n",
    "    \n",
    "    for df, cols_dict in columns_dict.items():\n",
    "\n",
    "        # List to save the relavant columns found in the df\n",
    "        cols_found = []\n",
    "\n",
    "        # Loop through all column names\n",
    "        for key, item in cols_dict.items():\n",
    "\n",
    "            # Check if required text is in colmn name,i.e. item\n",
    "            \n",
    "            # Ensure first that the text does not contain 'count' and 'sum' in it\n",
    "            if item.lower().find('sum'.lower()) == -1 and item.lower().find('count'.lower()) == -1:\n",
    "            \n",
    "                # When searching for Red, search for red but ignore RedEdge,i.e. RedE\n",
    "                if query.lower()=='red':\n",
    "                    if item.lower().find(query.lower()) != -1 and item.lower().find('RedE'.lower()) == -1:\n",
    "                        cols_found.append(item)\n",
    "\n",
    "                # For the rest also checking we do not get 'GNDVI' columns. Don't neet them yet\n",
    "                elif item.lower().find(query.lower()) != -1 and item.lower().find('GNDVI'.lower()) == -1:\n",
    "                    cols_found.append(item)\n",
    "\n",
    "\n",
    "        # Adding the found values to a column in dataframe\n",
    "        if len(cols_found) != num_std_dtype_colunms and query != 'id'.lower():\n",
    "            locals()[temp_df_non_std_cols][df] = pd.Series(cols_found)\n",
    "        \n",
    "        if len(cols_found) == num_std_dtype_colunms or query=='id'.lower():\n",
    "            locals()[temp_df_std_cols][df] = pd.Series(cols_found)\n",
    "            \n",
    "        if query=='id':\n",
    "            num_id_cols = len(cols_found)\n",
    "            if num_id_cols !=1:\n",
    "                if num_id_cols == 0:\n",
    "                    print('Error: ID column not found in' ,df)\n",
    "                else:\n",
    "                    print('Error: More than one ID column found in ' ,df)\n",
    "    \n",
    "    # Dropping the query column from the dataframe\n",
    "    locals()[temp_df_non_std_cols].drop(query, axis=1, inplace=True)\n",
    "    locals()[temp_df_std_cols].drop(query, axis=1, inplace=True)\n",
    "    \n",
    "    # Drop extra rows if the query is id\n",
    "    # Assumption: There is only one id columns in each df\n",
    "    # Assumption being checked in previous loop\n",
    "    if query =='id':\n",
    "        locals()[temp_df_std_cols].drop([1, 2], axis=0, inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # ===========================Start_of_Analysis_1================================\n",
    "#     # Analysis Part: Uncomment for new data\n",
    "#     # Confirm if all row elements corresopnd to data aggregation types in the dict\n",
    "    \n",
    "    \n",
    "#     agg_types_dict = {0:'mea', 1:'med', 2:'st'}\n",
    "    \n",
    "#     if query =='id':\n",
    "#         pass\n",
    "#     else:\n",
    "#         for key, agg_type in agg_types_dict.items():\n",
    "#             check_df = locals()[temp_df_std_cols]#.drop(query, axis=1)\n",
    "#             for row_item in check_df.iloc[key].tolist():\n",
    "#                 if row_item.lower().find(agg_type) == -1:\n",
    "#                     print(row_item, key, agg_type)\n",
    "                \n",
    "#     print(query, locals()[temp_df_std_cols].shape)\n",
    "#     display(locals()[temp_df_std_cols])\n",
    "    \n",
    "#     # Analysis shows that for df with num_std_dtype_colunms relavant column names; \n",
    "#     # green and RedEdge columns have naming problems where the agg_types are\n",
    "#     # not the same for mea and med for some green columns\n",
    "#     # and mea, med, and st for most of the RedEdge columns\n",
    "#     # =============================End_of_Analysis_1================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ===========================Start_of_Analysis_2================================\n",
    "    # Analysis Part: Uncomment for new data\n",
    "    # Renaming the column names to standard names \n",
    "    # Renaming based on the rule that the first name is mean, second is median and \n",
    "    # the third is standard deviation.\n",
    "    # This assumption has been partially checked and confirmed in Analysis_1\n",
    "    \n",
    "    # The id column has to be dealed separately since it does not have \n",
    "    # num_std_dtype_colunms variations i.e. mean, median, and std_dev\n",
    "    df_found_correct_cols = locals()[temp_df_std_cols].shape[1]\n",
    "    renamed_count = 0\n",
    "    if query=='id':\n",
    "        for x, y in locals()[temp_df_std_cols].items():\n",
    "            locals()[x].rename({y[0]: query_f}, axis=1, inplace=True)\n",
    "#             print(f'Renamed {query_f} column for dataframe {x}')\n",
    "            renamed_count += 1\n",
    "\n",
    "    else:\n",
    "        for x, y in locals()[temp_df_std_cols].items():\n",
    "            locals()[x].rename({y[0]: query_f+dtype_f[0], y[1]: query_f+dtype_f[1],\\\n",
    "                                y[2]: query_f+dtype_f[2]}, axis=1, inplace=True)\n",
    "#             print(f'Renamed {query_f} columns for dataframe {x}')\n",
    "            renamed_count += 1\n",
    "    print(f'{query_f} column renamed in {renamed_count} of {df_found_correct_cols} found dataframes')\n",
    "#     display(locals()[temp_df_std_cols].shape[2])\n",
    "    \n",
    "    # =============================End_of_Analysis_2================================\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # ===========================Start_of_Analysis_3================================\n",
    "#     # Analysis Part: Uncomment for new data\n",
    "#     # Checking the uncommon column names/combination or df with extra columns that \n",
    "#     # make finding the correct data confusing\n",
    "\n",
    "#     print(temp_df_non_std_cols)\n",
    "#     display(locals()[temp_df_non_std_cols])\n",
    "    \n",
    "#     # =============================End_of_Analysis_3================================\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of df with problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Graminor_070819_2',\n",
       " 'Graminor_220719_corrected',\n",
       " 'Graminor_east_010720',\n",
       " 'Graminor_east_090720',\n",
       " 'Graminor_east_250619_corrected',\n",
       " 'Graminor_west_020719',\n",
       " 'Masbasis_Mica_130720',\n",
       " 'Robot_Mica_070820',\n",
       " 'Robot_Mica_140820',\n",
       " 'Robot_Mica_160720'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "list_problem_df_per_query\n",
    "problem_df = []\n",
    "for df in list_problem_df_per_query:\n",
    "    problem_df.extend(locals()[df].columns)\n",
    "set(problem_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes with Problems\n",
    "\n",
    " 'Graminor_070819_2'  \n",
    " 'Graminor_220719_corrected'  \n",
    " 'Graminor_east_010720'  \n",
    " 'Graminor_east_090720'  \n",
    " 'Graminor_east_250619_corrected'  \n",
    " 'Graminor_west_020719'  \n",
    " 'Masbasis_Mica_130720'  \n",
    " 'Robot_Mica_070820'  \n",
    " 'Robot_Mica_140820'  \n",
    " 'Robot_Mica_160720'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of df with all required columns after standard names implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down standard column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "std_columns = []\n",
    "std_columns_mean = []\n",
    "std_columns_median = []\n",
    "std_columns_stdev = []\n",
    "\n",
    "for key, value in find_cols.items():\n",
    "    if key=='id':\n",
    "        std_columns.append(value)\n",
    "        std_columns_mean.append(value)\n",
    "        std_columns_median.append(value)\n",
    "        std_columns_stdev.append(value)\n",
    "\n",
    "    else:\n",
    "        std_columns.extend([value+y for x, y in dtype_f.items()])\n",
    "        std_columns_mean.append(value+dtype_f[0])\n",
    "        std_columns_median.append(value+dtype_f[1])\n",
    "        std_columns_stdev.append(value+dtype_f[2])\n",
    "        \n",
    "# std_columns\n",
    "# std_columns_mean\n",
    "# std_columns_median\n",
    "# std_columns_stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Columns Names  \n",
    "'Plot_ID'  \n",
    "'Blue_Mean_Value'  \n",
    "'Blue_Median_Value'  \n",
    "'Blue_Std_Dev_Value'  \n",
    "'Green_Mean_Value'  \n",
    "'Green_Median_Value'  \n",
    "'Green_Std_Dev_Value'  \n",
    "'Red_Mean_Value'  \n",
    "'Red_Median_Value'  \n",
    "'Red_Std_Dev_Value'  \n",
    "'RedEdge_Mean_Value'  \n",
    "'RedEdge_Median_Value'  \n",
    "'RedEdge_Std_Dev_Value'  \n",
    "'NIR_Mean_Value'  \n",
    "'NIR_Median_Value'  \n",
    "'NIR_Std_Dev_Value'  \n",
    "'NDVI_Mean_Value'  \n",
    "'NDVI_Median_Value'  \n",
    "'NDVI_Std_Dev_Value'  \n",
    "'MTCI_Mean_Value'  \n",
    "'MTCI_Median_Value'  \n",
    "'MTCI_Std_Dev_Value'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find df which have all the std columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 complete dataframes. Can be accessed using \"complete_dataframes\" list\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "count = 0\n",
    "complete_dataframes = []\n",
    "for df in all_df:\n",
    "    df_columns = locals()[df].columns\n",
    "    if (all(x in df_columns for x in std_columns)):\n",
    "        complete_dataframes.append(df)\n",
    "        count += 1\n",
    "print(f'Found {count} complete dataframes. Can be accessed using \\\"complete_dataframes\\\" list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing the names and dropping extra columns from complete dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "elements_to_strip = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_-().\"\n",
    "\n",
    "complete_df_std = []\n",
    "\n",
    "for df in complete_dataframes:\n",
    "    \n",
    "    # Getting date from the df name\n",
    "    date1 = df\n",
    "\n",
    "    for x in range(3):\n",
    "        date1 = date1.rstrip(elements_to_strip)\n",
    "        date1 = date1.lstrip(elements_to_strip)\n",
    "        for c in range(3):\n",
    "            date1 = date1.rstrip(elements_to_strip)\n",
    "            date1 = date1.lstrip(elements_to_strip)\n",
    "    \n",
    "    field_name = df.split('_')[0]\n",
    "    field_name = field_name.split('-')[0]\n",
    "\n",
    "    new_df_name = field_name +'_'+date1\n",
    "    \n",
    "    # Drop all columns except the std columns \n",
    "    locals()[new_df_name] = locals()[df][std_columns]\n",
    "    \n",
    "    complete_df_std.append(new_df_name)\n",
    "    \n",
    "#     print(new_df_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Find grain yield, DH, DT etc values and attach to the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Mean, Median and StdDev datasets with new column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "general_col_names = ['Plot_ID', 'Blue', 'Green', 'Red', 'RedEdge', 'NIR', 'NDVI', 'MTCI']\n",
    "\n",
    "df_all_mean = []\n",
    "df_all_median = []\n",
    "df_all_stdev = []\n",
    "\n",
    "for df in complete_df_std:\n",
    "    temp_mean_df = df+'_mean'\n",
    "    temp_median_df = df+'_median'\n",
    "    temp_stdev_df = df+'_stdev'\n",
    "    \n",
    "    # Filtering columns for each type\n",
    "    locals()[temp_mean_df] = locals()[df][std_columns_mean]\n",
    "    locals()[temp_median_df] = locals()[df][std_columns_median]\n",
    "    locals()[temp_stdev_df] = locals()[df][std_columns_stdev]\n",
    "    \n",
    "    # Renaming column names to general names for all\n",
    "    locals()[temp_mean_df].columns = general_col_names\n",
    "    locals()[temp_median_df].columns = general_col_names\n",
    "    locals()[temp_stdev_df].columns = general_col_names\n",
    "    \n",
    "    \n",
    "    df_all_mean.append(temp_mean_df)\n",
    "    df_all_median.append(temp_median_df)\n",
    "    df_all_stdev.append(temp_stdev_df)\n",
    "\n",
    "final_df = df_all_mean + df_all_median + df_all_stdev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate more indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the date from the name of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "list_df_all_indices = []\n",
    "\n",
    "for df in final_df:\n",
    "    new_df_name = df + '_all_indices'\n",
    "    \n",
    "    temp_df = locals()[df].copy()\n",
    "    \n",
    "    ######indices definition\n",
    "    temp_df['DVI']=temp_df['NIR']-temp_df['Red']\n",
    "    temp_df['GDVI']=temp_df['NIR']-temp_df['Green']\n",
    "    temp_df['MTCI_CI']=(temp_df['NIR']-temp_df['RedEdge'])/(temp_df['RedEdge']-temp_df['Red'])\n",
    "    temp_df['EXG']=(2*temp_df['Green'])-temp_df['Red']-temp_df['Blue']\n",
    "    temp_df['EXGR']=(3*temp_df['Green'])-(2.4*temp_df['Red'])-temp_df['Blue']\n",
    "\n",
    "\n",
    "    temp_df['RDVI']=(temp_df['NIR']-temp_df['Red'])/np.sqrt(temp_df[['NIR','Red']].sum(axis=1))\n",
    "    temp_df['TDVI']=1.5*(temp_df['NIR']-temp_df['Red'])/np.sqrt((np.power(temp_df['NIR'],2)+ temp_df['Red']+0.5 ))\n",
    "    temp_df['GNDVI']=(temp_df['NIR']-temp_df['Green'])/(temp_df['NIR']+temp_df['Green'])\n",
    "    temp_df['NDRE']=(temp_df['NIR']-temp_df['RedEdge'])/(temp_df['NIR']+temp_df['RedEdge'])\n",
    "    temp_df['SCCI']=temp_df['NDRE']/temp_df['NDVI']\n",
    "    temp_df['EVI']=2.5*(temp_df['NIR']-temp_df['Red'])/(temp_df['NIR']-6*(temp_df['Red'])-(7.5*temp_df['Blue'])-1)\n",
    "    temp_df['TVI']=0.5*(120*(temp_df['NIR']-temp_df['Green'])-200*(temp_df['Red']-temp_df['Green']))\n",
    "    temp_df['VARI']=(temp_df['Green']-temp_df['Red'])/(temp_df['Green']+temp_df['Red']-temp_df['Blue'])\n",
    "    temp_df['GARI']=(temp_df['NIR']-temp_df['Green'])-(1.7*(temp_df['Blue']-temp_df['Red']))/(temp_df['NIR']+temp_df['Green'])-(1.7*(temp_df['Blue']-temp_df['Red']))\n",
    "    temp_df['GCI']=(temp_df['NIR']/temp_df['Green'])-1\n",
    "    temp_df['GLI']=(temp_df['Green']-temp_df['Red']-temp_df['Blue'])/(2*temp_df['Green']+temp_df['Red']+temp_df['Blue'])\n",
    "    temp_df['NLI']=(np.power(temp_df['NIR'],2)-temp_df['Red'])/(np.power(temp_df['NIR'],2)+temp_df['Red'])\n",
    "    temp_df['MNLI']=(np.power(temp_df['NIR'],2)-temp_df['Red'])*1.5/(np.power(temp_df['NIR'],2)+temp_df['Red'] + 0.5)\n",
    "    temp_df['SAVI']= ((temp_df['NIR']-temp_df['Red'])*1.5)/(temp_df['NIR']+ temp_df['Red']+ 0.5) \n",
    "    temp_df['GSAVI']= ((temp_df['NIR']-temp_df['Green'])*1.5)/(temp_df['NIR']+ temp_df['Green']+ 0.5)                                    \n",
    "    temp_df['OSAVI']= ((temp_df['NIR']-temp_df['Red']))/(temp_df['NIR']+ temp_df['Red']+ 0.16)\n",
    "    temp_df['GOSAVI']= ((temp_df['NIR']-temp_df['Green']))/(temp_df['NIR']+ (temp_df['Green'])+ 0.16)\n",
    "    temp_df['MSAVI2']=(2*temp_df['NIR'])+1-np.sqrt(np.power((2*temp_df['NIR']+1),2)-8*(temp_df['NIR']-temp_df['Red']))/2\n",
    "    temp_df['MSR']=(temp_df['NIR']/temp_df['Red'])-(1/np.sqrt(temp_df['NIR']/temp_df['Red']))\n",
    "    temp_df['GRVI']=(temp_df['NIR']/temp_df['Green'])\n",
    "    temp_df['WDRVI']=((0.1*temp_df['NIR'])-temp_df['Red'])/((0.1*temp_df['NIR'])+temp_df['Red'])\n",
    "    temp_df['SR']=(temp_df['NIR']/temp_df['Red'])\n",
    "    temp_df['Time']=pd.to_datetime(date1, infer_datetime_format= True)\n",
    "\n",
    "    \n",
    "    list_df_all_indices.append(new_df_name)\n",
    "    locals()[new_df_name] = temp_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 36)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Graminor_150819_mean_all_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add date as column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import date\n",
    "M_180620_MICA_MEDIAN['Time']=pd.to_datetime('2020-06-18'.replace(\"'\",\"\"))\n",
    "\n",
    "\n",
    "import datetime\n",
    "from datetime import date\n",
    "M_180620_MICA_MEAN['Time']=pd.to_datetime('2020-06-18'.replace(\"'\",\"\"))\n",
    "M_180620_MICA_MEDIAN['Time']=pd.to_datetime('2020-06-18'.replace(\"'\",\"\"))\n",
    "###Write to excel\n",
    "with pd.ExcelWriter('Masbasis_Mica_Indices_180620.xlsx') as writer:\n",
    "    M_180620_MICA_MEAN. to_excel(writer, sheet_name='mean')\n",
    "    M_180620_MICA_MEDIAN. to_excel(writer, sheet_name='median')\n",
    "writer.save()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add date with field+Year as column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Masbasis_Mica_2020_all_dates_MEDIAN[['Time','MTCI_CI']].boxplot(by='Time', figsize=(10,6))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####loading dateset \n",
    "M_180620_Mica=pd.read_excel('Masbasis_Mica_Indices_180620.xlsx', None, engine='openpyxl')\n",
    "# len(M_240620_Mica.columns)\n",
    "M_180620_Mica.keys()\n",
    "type(M_180620_Mica)\n",
    "M_180620_Mica['mean']\n",
    "type(M_180620_Mica['median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###mean valuse\n",
    "M_180620_MICA_MEAN = M_180620_Mica['mean'].dropna()\n",
    "M_180620_MICA_MEDIAN = M_180620_Mica['median'].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_180620_MICA_MEAN_1 = pd.DataFrame(M_180620_MICA_MEAN,columns=['Blue', 'Green', 'Red','RedEdge', 'NIR','NDVI', 'MTCI'])\n",
    "M_180620_MICA_MEAN_1\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer \n",
    "\n",
    "pt = PowerTransformer(method='box-cox', standardize=False)\n",
    "M_180620_MICA_MEAN_PT = pt.fit_transform(M_180620_MICA_MEAN_1)\n",
    "\n",
    "M_180620_MICA_MEAN_PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for col in ['Blue', 'Green', 'Red','RedEdge', 'NIR','NDVI', 'MTCI']:\n",
    "pd.DataFrame(M_180620_MICA_MEAN_PT).boxplot(figsize=(15,8))\n",
    "2019,\n",
    "2020\n",
    "Masbasis\n",
    "Use AOC\n",
    "Remove dates which have drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_180620_MICA_MEAN_1 = pd.DataFrame(M_180620_MICA_MEAN,columns=['Blue', 'Green', 'Red','RedEdge', 'NIR','NDVI', 'MTCI'])\n",
    "M_180620_MICA_MEAN.boxplot(figsize=(10,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####concating all dates in one file\n",
    "Masbasis_Mica_2020_all_dates_mean=pd.concat([M_180620_MICA_MEAN,M_240620_MICA_MEAN, M_260620_MICA_MEAN, M_010720_MICA_MEAN,\n",
    "                                            M_080720_MICA_MEAN,M_130720_MICA_MEAN,M_170720_MICA_MEAN, M_220720_MICA_MEAN,\n",
    "                                             M_300720_MICA_MEAN, M_070820_MICA_MEAN, M_120820_MICA_MEAN, M_140820_MICA_MEAN],axis=0, sort=True)\n",
    "Masbasis_Mica_2020_all_dates_mean.to_excel('Masbasis_Mica_2020_all_dates_mean.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####concating all dates in one file\n",
    "Masbasis_Mica_2020_all_dates_MEDIAN=pd.concat([M_180620_MICA_MEDIAN,M_240620_MICA_MEDIAN, M_260620_MICA_MEDIAN, M_010720_MICA_MEDIAN,\n",
    "                                            M_080720_MICA_MEDIAN,M_130720_MICA_MEDIAN,M_170720_MICA_MEDIAN, M_220720_MICA_MEDIAN,\n",
    "                                             M_300720_MICA_MEDIAN, M_070820_MICA_MEDIAN, M_120820_MICA_MEDIAN, M_140820_MICA_MEDIAN],axis=0, sort=True)\n",
    "Masbasis_Mica_2020_all_dates_MEDIAN.to_excel('Masbasis_Mica_2020_all_dates_MEDIAN.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Masbasis_Mica_2020_all_dates_MEDIAN[['Time','NDVI']].boxplot(by='Time', figsize=(10,6))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Masbasis_Mica_2020_all_dates_MEDIAN[['Time','NIR']].boxplot(by='Time', figsize=(10,6))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Masbasis_Mica_2020_all_dates_MEDIAN[['Time','MTCI_CI']].boxplot(by='Time', figsize=(10,6))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Masbasis_Mica_2020_all_dates_MEDIAN[['Time','Blue']].boxplot(by='Time', figsize=(10,6))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Masbasis_Mica_2020_all_dates_MEDIAN[['Time','MTCI']].boxplot(by='Time', figsize=(10,6))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Masbasis_Mica_2020_all_dates_MEDIAN[['Time','Green']].boxplot(by='Time', figsize=(10,6))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Masbasis_Mica_2020_all_dates_MEDIAN[['Time','Red']].boxplot(by='Time', figsize=(10,6))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Masbasis_Mica_2020_all_dates_MEDIAN[['Time','RedEdge']].boxplot(by='Time', figsize=(10,6))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Masbasis_Mica_2020_all_dates_MEDIAN[['Time','EXG']].boxplot(by='Time', figsize=(10,6))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df=pd.read_excel('Masbasis_Mica_2020_all_dates_MEDIAN.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
