{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Masbaisis_indices_280619.xlsx to Masbasis_indices_280619.xlsx  \n",
    "Rename Masbasis_2606_2019_color_and_othe_indecies  to Masbasis_260619_color_and_othe_indecies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:47:44.476962Z",
     "start_time": "2021-09-08T15:47:43.277869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "from copy import copy\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To display df nicely in loops\n",
    "from IPython.display import display \n",
    "# display(df1.head()) \n",
    "# display(df2.head())\n",
    "\n",
    "# Display rows and columns Pandas\n",
    "pd.options.display.max_columns = 100\n",
    "pd.set_option('display.max_rows',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:47:44.491919Z",
     "start_time": "2021-09-08T15:47:44.478856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\fahad\\\\MegaSync\\\\NMBU\\\\GitHub\\\\vPheno'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints the current working directory\n",
    "os.getcwd()\n",
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Username folder to make general path for multi PC use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:47:44.506885Z",
     "start_time": "2021-09-08T15:47:44.493920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fahad'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username = str(os.getcwd()).split('\\\\')[2]\n",
    "username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo:  \n",
    "  \n",
    "2019 2020 data is fine  \n",
    "Cant use 2017 because of the blue band  \n",
    "Distt is not normal in 2018 robot  \n",
    "AREA UNDER THE CURVE in each season for 2018 2017  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing Manual work to involve grain yield\n",
    "## ToDo: Must automate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:47:44.522843Z",
     "start_time": "2021-09-08T15:47:44.508880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Graminor_2019', 'Masbasis_2019', 'Staur_2019', 'Vollebekke-total_2020']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'C:\\\\Users\\\\'+username+'\\\\MegaSync\\\\NMBU\\\\Master Thesis\\\\Data\\\\Feb2021'\n",
    "export_path = './Data/'\n",
    "complete_df_path = export_path+'complete/'\n",
    "incomplete_df_path = export_path+'incomplete/'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Creating list of all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:47:44.537802Z",
     "start_time": "2021-09-08T15:47:44.524838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 files founnd in the directory\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all files in directory tree at given path\n",
    "\n",
    "files_with_address = []\n",
    "files_list = []\n",
    "\n",
    "for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "    files_with_address += [os.path.join(dirpath, file) for file in filenames]\n",
    "    files_list.extend(filenames)\n",
    "    \n",
    "print(len(files_with_address), 'files founnd in the directory')\n",
    "# files_with_address\n",
    "# files_list\n",
    "files_with_address_bkp = copy(files_with_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Checking/control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "### Check for duplicate filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:47:44.552763Z",
     "start_time": "2021-09-08T15:47:44.539798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files are : 74\n",
      "Number of unique file names are: 73\n",
      "There is/are 1 duplicate file name/names.\n"
     ]
    }
   ],
   "source": [
    "print('Total number of files are :', len(files_list))\n",
    "\n",
    "print('Number of unique file names are:', len(set(files_list)))\n",
    "\n",
    "print('There is/are', len(files_list) - len(set(files_list)),'duplicate file name/names.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if there are multiple sheets in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:47:47.011047Z",
     "start_time": "2021-09-08T15:47:44.555755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following files have multiple sheets.\n",
      "4 19TvPhenores.xlsx in folder Graminor_2019\n",
      "4 19TvPhenores pedigree.xlsx in folder Staur_2019\n",
      "4 19TvPhenores.xlsx in folder Staur_2019\n",
      "3 20BMLGI1_2020_tm.xlsx in folder Masbasis\n",
      "3 Masbasis_Mica_2020_all_dates_MEDIAN_DP.xlsx in folder Masbasis\n",
      "3 ROBOT_2020.xlsx in folder Robot\n"
     ]
    }
   ],
   "source": [
    "# Print number of sheets in all files\n",
    "print('The following files have multiple sheets.')\n",
    "\n",
    "list_multi_sheet = []\n",
    "for file in files_with_address:\n",
    "    \n",
    "    xl_file = pd.ExcelFile(file,engine='openpyxl')\n",
    "    number_of_sheets = len(xl_file.sheet_names)\n",
    "    if number_of_sheets > 1:\n",
    "        print(number_of_sheets, os.path.basename(file), 'in folder', os.path.basename(os.path.dirname(file))\n",
    ")\n",
    "        list_multi_sheet.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing files with multiple sheets from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:47:47.027102Z",
     "start_time": "2021-09-08T15:47:47.014043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing files with multiple sheets from the list\n",
    "\n",
    "for i in list_multi_sheet:\n",
    "    files_with_address.remove(i)\n",
    "len(files_with_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing files without dates\n",
    "(with 2019 in name means they dont have date format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:47:47.042632Z",
     "start_time": "2021-09-08T15:47:47.029098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field_data_2019.xlsx in folder Masbasis_2019\n",
      "Masbasis_2019_ForDP_median.xlsx in folder Masbasis_2019\n",
      "NEW_Field_data_2019.xlsx in folder Masbasis_2019\n",
      "Staur_Graminor_2019_median.xlsx in folder Staur_2019\n",
      "Staur_Masbasis_2019.xlsx in folder Staur_2019\n"
     ]
    }
   ],
   "source": [
    "files_w_2019 = []\n",
    "for file in files_with_address:\n",
    "    file_name = os.path.basename(file)\n",
    "    if '2019' in file_name:\n",
    "        print(file_name, 'in folder', os.path.basename(os.path.dirname(file)))\n",
    "        files_w_2019.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:47:47.058592Z",
     "start_time": "2021-09-08T15:47:47.044631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\\\fahad\\MegaSync\\NMBU\\Master Thesis\\Data\\Feb2021\\Masbasis_2019\\Field_data_2019.xlsx\n",
      "C:\\\\Users\\\\fahad\\MegaSync\\NMBU\\Master Thesis\\Data\\Feb2021\\Masbasis_2019\\Masbasis_2019_ForDP_median.xlsx\n",
      "C:\\\\Users\\\\fahad\\MegaSync\\NMBU\\Master Thesis\\Data\\Feb2021\\Masbasis_2019\\NEW_Field_data_2019.xlsx\n",
      "C:\\\\Users\\\\fahad\\MegaSync\\NMBU\\Master Thesis\\Data\\Feb2021\\Staur_2019\\Staur_Graminor_2019_median.xlsx\n",
      "C:\\\\Users\\\\fahad\\MegaSync\\NMBU\\Master Thesis\\Data\\Feb2021\\Staur_2019\\Staur_Masbasis_2019.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Removing from list\n",
    "for i in files_w_2019:\n",
    "    print(i)\n",
    "    files_with_address.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:47:47.074441Z",
     "start_time": "2021-09-08T15:47:47.059589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_with_address)\n",
    "# files_with_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking number of unique cultivars in the field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:47:47.090401Z",
     "start_time": "2021-09-08T15:47:47.076439Z"
    }
   },
   "outputs": [],
   "source": [
    "# plots_data = pd.read_excel(files_with_address[0],engine='openpyxl')\n",
    "# # Pandas converts 'NA' string to NaN. Need to change those to \n",
    "# # some string to get a count as NaNs are not counted as unique values\n",
    "\n",
    "# plots_data.Name.fillna('-', inplace=True)\n",
    "# plots_data.CodeName.fillna('-', inplace=True)\n",
    "\n",
    "# # Creating a new column as multiple plots were named 'NA' but the \n",
    "# # CodeName was different for each one of them\n",
    "# plots_data['NameCode'] = plots_data.Name+plots_data.CodeName\n",
    "\n",
    "# plots_data\n",
    "# len(plots_data.NameCode.unique())\n",
    "# plots_data.NameCode.value_counts()\n",
    "# # plots_data.NameCode.value_counts().sum()\n",
    "# # plots_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data files to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:48:09.591643Z",
     "start_time": "2021-09-08T15:47:47.091398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graminor_070819_2 ===== (600, 49)\n",
      "Graminor_150819 ===== (600, 49)\n",
      "Graminor_eastwest_050819 ===== (600, 49)\n",
      "Graminor_250719 ===== (600, 46)\n",
      "Graminor_050719_one_missing_row_deleted ===== (600, 49)\n",
      "Graminor_150719 ===== (600, 49)\n",
      "Graminor_220719_corrected ===== (600, 49)\n",
      "Graminor_eastwest_020719_NIR_half_missing ===== (600, 22)\n",
      "Graminor_east_110719 ===== (300, 49)\n",
      "Graminor_060619 ===== (600, 49)\n",
      "Graminor_110619 ===== (600, 49)\n",
      "Graminor_280619_corrected ===== (600, 49)\n",
      "Graminor_eastwest_250619_correct_duplicate_east ===== (600, 97)\n",
      "Masbasis_050719_corrected ===== (528, 49)\n",
      "Masbasis_060619_Indices ===== (528, 56)\n",
      "Masbasis_070819_correct ===== (528, 49)\n",
      "Masbasis_110619 ===== (528, 82)\n",
      "Masbasis_150719 ===== (528, 49)\n",
      "Masbasis_220719_correct ===== (528, 49)\n",
      "Masbasis_260619_color_and_othe_indecies ===== (528, 47)\n",
      "Masbasis_290719 ===== (528, 49)\n",
      "Masbasis_indices_280619 ===== (528, 49)\n",
      "Graminor_eastwest_040720 ===== (800, 49)\n",
      "Graminor_eastwest_040820 ===== (793, 22)\n",
      "Graminor_eastwest_070720_correct ===== (800, 49)\n",
      "Graminor_eastwest_090720_duplication_same_as_010720 ===== (800, 99)\n",
      "Graminor_eastwest_130720 ===== (800, 22)\n",
      "Graminor_eastwest_140820 ===== (800, 49)\n",
      "Graminor_eastwest_300720 ===== (787, 22)\n",
      "Graminor_east_010720 ===== (400, 51)\n",
      "Graminor_east_170720cpg ===== (400, 49)\n",
      "Graminor_east_180620 ===== (400, 46)\n",
      "Graminor_east_200720 ===== (400, 22)\n",
      "Graminor_Mica_eastcorrect_west_240620 ===== (757, 49)\n",
      "Masbasis_Mica_070820 ===== (688, 49)\n",
      "Masbasis_Mica_120820 ===== (688, 49)\n",
      "Masbasis_Mica_140820 ===== (688, 49)\n",
      "Masbasis_Mica_010720 ===== (688, 49)\n",
      "Masbasis_Mica_080720 ===== (688, 22)\n",
      "Masbasis_Mica_130720 ===== (688, 50)\n",
      "Masbasis_mica_170720 ===== (688, 23)\n",
      "Masbasis_Mica_220720 ===== (688, 49)\n",
      "Masbasis_Mica_300720_duplicate_plots_deleted_1332,1329,1330,1331same_data ===== (688, 49)\n",
      "Masbasis_mica_180620_several_missing_rows_deleted ===== (688, 46)\n",
      "Masbasis_mica_240620 ===== (688, 22)\n",
      "Masbasis_Mica_260620 ===== (688, 46)\n",
      "Robot_Mica_010720 ===== (96, 22)\n",
      "Robot_Mica_040820 ===== (96, 49)\n",
      "Robot_Mica_070720 ===== (96, 22)\n",
      "Robot_Mica_070820 ===== (96, 97)\n",
      "Robot_Mica_120820 ===== (96, 22)\n",
      "Robot_Mica_130720 ===== (96, 22)\n",
      "Robot_Mica_140820 ===== (96, 40)\n",
      "Robot_Mica_160720 ===== (96, 19)\n",
      "Robot_mica_180620 ===== (96, 49)\n",
      "Robot_Mica_200720 ===== (96, 49)\n",
      "Robot_Mica_220720 ===== (96, 49)\n",
      "Robot_Mica_230620 ===== (96, 46)\n",
      "Robot_Mica_240620 ===== (96, 49)\n",
      "Robot_Mica_250620 ===== (96, 46)\n",
      "Robot_Mica_270720 ===== (96, 49)\n",
      "Robot_Mica_290620 ===== (96, 49)\n",
      "Robot_Mica_300720 ===== (96, 22)\n",
      "Wall time: 22.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_df = []\n",
    "for data in files_with_address:\n",
    "    file_name = os.path.splitext(os.path.basename(data))[0]\n",
    "\n",
    "    # Replce all invalid characters in the name\n",
    "    file_name = file_name.replace(\"-\", \"_\")\n",
    "    file_name = file_name.replace(\" \", \"_\")\n",
    "    file_name = file_name.replace(\"(\", \"\")\n",
    "    file_name = file_name.replace(\")\", \"\")\n",
    "    df_name = file_name.replace(\".\", \"\")\n",
    "    # Test: Check if the same date is already present in the current dict key\n",
    "    if df_name in all_df:\n",
    "        print(f'A file with the same name {df_name} has already been imported. \\n Please check if there is duplication of data.')\n",
    "        raise NameError\n",
    "            \n",
    "    all_df.append(df_name)\n",
    "\n",
    "    locals()[df_name] = pd.read_excel(data, engine='openpyxl')\n",
    "    print(df_name, '=====', locals()[df_name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:48:09.607604Z",
     "start_time": "2021-09-08T15:48:09.595637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total imported 63\n"
     ]
    }
   ],
   "source": [
    "print(f'Total imported {len(all_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Control: Check which df have the data column heading we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing column headings into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:48:09.671430Z",
     "start_time": "2021-09-08T15:48:09.609600Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Finding max number of columns in all df\n",
    "\n",
    "len_columns = []\n",
    "for df in all_df:\n",
    "    cols_df = locals()[df].columns\n",
    "    len_columns.append(len(cols_df))\n",
    "max_cols_in_df = max(len_columns)\n",
    "\n",
    "# Now creating a empty df to collect all column headings\n",
    "columns_df = pd.DataFrame(data=range(0,max_cols_in_df), columns = ['ID'])\n",
    "columns_df.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "for df in all_df:\n",
    "    cols_df = locals()[df].columns\n",
    "    columns_df[df] = pd.Series(cols_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert columns_df to dictoionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:48:09.701063Z",
     "start_time": "2021-09-08T15:48:09.672426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Function to convert df to dict while dropping nan in each column separately\n",
    "\n",
    "def comp_dropna(df1):\n",
    "    return {k: v.dropna().to_dict() for k,v in df1.items()}\n",
    "\n",
    "columns_dict = comp_dropna(columns_df)\n",
    "# columns_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Column heading with the text we are looking for in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:48:10.141886Z",
     "start_time": "2021-09-08T15:48:09.703060Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 63 datasets have been imported.\n",
      "id => Plot_ID column renamed in 54 of 54 found dataframes\n",
      "plot => Plot_ID column renamed in 9 of 9 found dataframes\n",
      "Blue => Blue column renamed in 58 of 58 found dataframes\n",
      "Gre => Green column renamed in 58 of 58 found dataframes\n",
      "Red => Red column renamed in 58 of 58 found dataframes\n",
      "RedE => RedEdge column renamed in 58 of 58 found dataframes\n",
      "NIR => NIR column renamed in 57 of 57 found dataframes\n",
      "Wall time: 414 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(f'Total {len(all_df)} datasets have been imported.')\n",
    "\n",
    "# Find columns with key from the following list\n",
    "# Rename the founc columns to the values against the keys\n",
    "\n",
    "id_cols_var = ['id', 'plot']\n",
    "id_cols_new = ['Plot_ID']\n",
    "\n",
    "\n",
    "find_cols = {\n",
    "    id_cols_var[0]: id_cols_new[0],\n",
    "    id_cols_var[1]: id_cols_new[0],\n",
    "    'Blue': 'Blue',\n",
    "    'Gre': 'Green',\n",
    "    'Red': 'Red',\n",
    "    'RedE': 'RedEdge',\n",
    "    'NIR': 'NIR',\n",
    "#     'NDVI': 'NDVI',\n",
    "#     'MTCI': 'MTCI'\n",
    "}\n",
    "\n",
    "\n",
    "dtype_f = {0: '_Mean_Value', 1: '_Median_Value', 2: '_Std_Dev_Value'}\n",
    "num_std_dtype_colunms = len(dtype_f)\n",
    "\n",
    "list_ok_df_per_query = []\n",
    "list_problem_df_per_query = []\n",
    "\n",
    "for query, query_f in find_cols.items():\n",
    "    \n",
    "    # First, we loop through all df to find the max of number of relavant columns in any df\n",
    "    num_col_found_all = []\n",
    "    for df, cols_dict in columns_dict.items():\n",
    "\n",
    "        # List to save the relavant columns found in the df\n",
    "        cols_found = []\n",
    "        \n",
    "        # Loop through all column names\n",
    "        for key, item in cols_dict.items():\n",
    "            \n",
    "            # Check if required text is in colmn name,i.e. item\n",
    "            \n",
    "            # Ensure first that the text does not contain 'count' and 'sum' in it\n",
    "            if item.lower().find('sum'.lower()) == -1 and item.lower().find('count'.lower()) == -1:\n",
    "            \n",
    "                # When searching for Red, search for red but ignore RedEdge,i.e. RedE\n",
    "                if query.lower()=='red':\n",
    "                    if item.lower().find(query.lower()) != -1 and item.lower().find('RedE'.lower()) == -1:\n",
    "                        cols_found.append(item)\n",
    "\n",
    "                # For the rest also checking we do not get 'GNDVI' columns. Don't neet them yet\n",
    "                elif item.lower().find(query.lower()) != -1 and item.lower().find('GNDVI'.lower()) == -1:\n",
    "                    cols_found.append(item)\n",
    "\n",
    "        num_col_found_all.append(len(cols_found))\n",
    "    max_cols = max(num_col_found_all)\n",
    "#     print(query, max_cols)\n",
    "\n",
    "    # Define a df for saving the columns found if num of col is not num_std_dtype_colunms\n",
    "    temp_df_non_std_cols = 'col_df_non_std_cols_'+query\n",
    "    \n",
    "    # Add a condition in data if no columns found, then still create a df with 1 column\n",
    "    locals()[temp_df_non_std_cols] = pd.DataFrame(data=range(0,max_cols) if max_cols > 1 else range(0,1),\n",
    "                                                  columns = [query])\n",
    "    list_problem_df_per_query.append(temp_df_non_std_cols)\n",
    "    \n",
    "    # Define a df for saving the columns found if num of col is num_std_dtype_colunms\n",
    "    temp_df_std_cols= 'col_df_std_cols_'+query\n",
    "    locals()[temp_df_std_cols] = pd.DataFrame(data=range(0,num_std_dtype_colunms), columns = [query])\n",
    "    list_ok_df_per_query.append(temp_df_std_cols)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Now we again loop and append those columns to a relavant df for each query\n",
    "    \n",
    "    for df, cols_dict in columns_dict.items():\n",
    "\n",
    "        # List to save the relavant columns found in the df\n",
    "        cols_found = []\n",
    "\n",
    "        # Loop through all column names\n",
    "        for key, item in cols_dict.items():\n",
    "\n",
    "            # Check if required text is in colmn name,i.e. item\n",
    "            \n",
    "            # Ensure first that the text does not contain 'count' and 'sum' in it\n",
    "            if item.lower().find('sum'.lower()) == -1 and item.lower().find('count'.lower()) == -1 and item.lower().find('GreenND'.lower()) == -1:\n",
    "            \n",
    "                # When searching for Red, search for red but ignore RedEdge,i.e. RedE\n",
    "                if query.lower()=='red':\n",
    "                    if item.lower().find(query.lower()) != -1 and item.lower().find('RedE'.lower()) == -1:\n",
    "                        cols_found.append(item)\n",
    "\n",
    "                # For the rest also checking we do not get 'GNDVI' columns. Don't neet them yet\n",
    "                elif item.lower().find(query.lower()) != -1 and item.lower().find('GNDVI'.lower()) == -1:\n",
    "                    cols_found.append(item)\n",
    "\n",
    "        # Adding the found values to a column in dataframe\n",
    "        \n",
    "        # For non standard columns\n",
    "        if len(cols_found) != num_std_dtype_colunms and query not in id_cols_var:\n",
    "            locals()[temp_df_non_std_cols][df] = pd.Series(cols_found, dtype='str')\n",
    "        \n",
    "        # For standrd columns\n",
    "        if len(cols_found) == num_std_dtype_colunms or query in id_cols_var:\n",
    "            locals()[temp_df_std_cols][df] = pd.Series(cols_found, dtype='str')\n",
    "            \n",
    "\n",
    "        if query in id_cols_var:\n",
    "            num_id_cols = 0\n",
    "            for temp_query in id_cols_var:\n",
    "                for column in cols_dict.values():\n",
    "                    if column.lower().find(temp_query.lower()) == 0:\n",
    "                        num_id_cols += 1\n",
    "            if num_id_cols !=1:\n",
    "                if num_id_cols == 0:\n",
    "                    print(query, 'Error: ID column not found in' ,df)\n",
    "                else:\n",
    "                    print('Error: More than one ID column found in ' ,df)\n",
    "    \n",
    "    # Dropping the empty query named column from the columns dataframe\n",
    "    # For non standard columns\n",
    "    locals()[temp_df_non_std_cols].drop(query, axis=1, inplace=True)\n",
    "    # For standrd columns\n",
    "    locals()[temp_df_std_cols].drop(query, axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    # Drop extra empty rows if the query is id or plot\n",
    "    # Drop empty columns, where the id is different than the one being checked\n",
    "    # Drop first column with \n",
    "    # Assumption: There is only one id columns in each df\n",
    "    # Assumption being checked in previous loop\n",
    "    if query in id_cols_var:\n",
    "        locals()[temp_df_std_cols].drop([1, 2], axis=0, inplace=True)\n",
    "        locals()[temp_df_std_cols].dropna(axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "#     # ===========================Start_of_Analysis_1================================\n",
    "#     # Analysis Part: Uncomment for new data\n",
    "#     # Confirm if all row elements corresopnd to data aggregation types in the dict\n",
    "    \n",
    "    \n",
    "#     agg_types_dict = {0:'mea', 1:'med', 2:'st'}\n",
    "    \n",
    "#     if query in id_cols_var:\n",
    "#         pass\n",
    "#     else:\n",
    "#         for key, agg_type in agg_types_dict.items():\n",
    "#             check_df = locals()[temp_df_std_cols]#.drop(query, axis=1)\n",
    "#             for row_item in check_df.iloc[key].tolist():\n",
    "#                 if str(row_item).lower().find(agg_type) == -1:\n",
    "#                     print(f'{row_item}, {key}, {agg_type}')\n",
    "                \n",
    "#     print(query, locals()[temp_df_std_cols].shape)\n",
    "#     display(locals()[temp_df_std_cols])\n",
    "    \n",
    "#     # Analysis shows that for df with num_std_dtype_colunms relavant column names; \n",
    "#     # green and RedEdge columns have naming problems where the agg_types are\n",
    "#     # not the same for mea and med for some green columns\n",
    "#     # and mea, med, and st for most of the RedEdge columns\n",
    "#     # =============================End_of_Analysis_1================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ===========================Start_of_Analysis_2================================\n",
    "    # Analysis Part: Uncomment for new data\n",
    "    # Renaming the column names to standard names \n",
    "    # Renaming based on the rule that the first name is mean, second is median and \n",
    "    # the third is standard deviation.\n",
    "    # This assumption has been partially checked and confirmed in Analysis_1\n",
    "    \n",
    "    # The id column has to be dealed separately since it does not have \n",
    "    # num_std_dtype_colunms variations i.e. mean, median, and std_dev\n",
    "    df_found_correct_cols = locals()[temp_df_std_cols].shape[1]\n",
    "    renamed_count = 0\n",
    "    if query in id_cols_var:\n",
    "        for x, y in locals()[temp_df_std_cols].items():\n",
    "            locals()[x].rename({y[0]: query_f}, axis=1, inplace=True)\n",
    "#             print(f'Renamed {query_f} column for dataframe {x} against value {y[0]}')\n",
    "            renamed_count += 1\n",
    "\n",
    "    else:\n",
    "        for x, y in locals()[temp_df_std_cols].items():\n",
    "            locals()[x].rename({y[0]: query_f+dtype_f[0], y[1]: query_f+dtype_f[1],\\\n",
    "                                y[2]: query_f+dtype_f[2]}, axis=1, inplace=True)\n",
    "#             print(f'Renamed {query_f} columns for dataframe {x}')\n",
    "            renamed_count += 1\n",
    "    print(f'{query} => {query_f} column renamed in {renamed_count} of {df_found_correct_cols} found dataframes')\n",
    "#     display(locals()[temp_df_std_cols].shape[2])\n",
    "    \n",
    "    # =============================End_of_Analysis_2================================\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # ===========================Start_of_Analysis_3================================\n",
    "#     # Analysis Part: Uncomment for new data\n",
    "#     # Checking the uncommon column names/combination or df with extra columns that \n",
    "#     # make finding the correct data confusing\n",
    "\n",
    "#     print(temp_df_non_std_cols)\n",
    "#     display(locals()[temp_df_non_std_cols])\n",
    "\n",
    "#     # =============================End_of_Analysis_3================================\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of df with problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:48:10.157377Z",
     "start_time": "2021-09-08T15:48:10.142884Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Wall time: 999 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Graminor_220719_corrected',\n",
       " 'Graminor_eastwest_090720_duplication_same_as_010720',\n",
       " 'Graminor_eastwest_250619_correct_duplicate_east',\n",
       " 'Masbasis_110619',\n",
       " 'Robot_Mica_070820',\n",
       " 'Robot_Mica_140820',\n",
       " 'Robot_Mica_160720'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "list_problem_df_per_query\n",
    "problem_df = []\n",
    "for df in list_problem_df_per_query:\n",
    "    problem_df.extend(locals()[df].columns)\n",
    "\n",
    "print(len(set(problem_df)))\n",
    "set(problem_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probles after merging eastwest in Graminor  \n",
    " 'Graminor_220719_corrected' NIR Missing  \n",
    " 'Graminor_eastwest_090720_duplication-same_as_010720' duplication  \n",
    " 'Graminor_eastwest_250619_correct_duplicate_east' duplication  \n",
    " 'Masbasis_110619' duplication  \n",
    " 'Robot_Mica_070820' duplication  \n",
    " 'Robot_Mica_140820' duplication  \n",
    " 'Robot_Mica_160720' NIR Missing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem datasets before combining east west datasets in Graminor{'Graminor_220719_corrected',  \n",
    " 'Graminor_east_090720'  \n",
    " 'Graminor_east_250619_corrected' \n",
    " 'Graminor_west_020719'   \n",
    " 'Masbasis_110619'   \n",
    " 'Robot_Mica_070820'   \n",
    " 'Robot_Mica_140820'   \n",
    " 'Robot_Mica_160720'\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes with Problems (begining)\n",
    "\n",
    " 'Graminor_070819_2'  \n",
    " 'Graminor_220719_corrected'  \n",
    " 'Graminor_east_010720'  \n",
    " 'Graminor_east_090720'  \n",
    " 'Graminor_east_250619_corrected'  \n",
    " 'Graminor_west_020719'  \n",
    " 'Masbasis_Mica_130720'  \n",
    " 'Robot_Mica_070820'  \n",
    " 'Robot_Mica_140820'  \n",
    " 'Robot_Mica_160720'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardise the column names\n",
    "Get list of df with all required columns after implementing standard names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo: Test: Check if there is duplication of column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List down standard column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:48:10.173329Z",
     "start_time": "2021-09-08T15:48:10.161366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Plot_ID',\n",
       " 'Blue_Mean_Value',\n",
       " 'Blue_Median_Value',\n",
       " 'Blue_Std_Dev_Value',\n",
       " 'Green_Mean_Value',\n",
       " 'Green_Median_Value',\n",
       " 'Green_Std_Dev_Value',\n",
       " 'Red_Mean_Value',\n",
       " 'Red_Median_Value',\n",
       " 'Red_Std_Dev_Value',\n",
       " 'RedEdge_Mean_Value',\n",
       " 'RedEdge_Median_Value',\n",
       " 'RedEdge_Std_Dev_Value',\n",
       " 'NIR_Mean_Value',\n",
       " 'NIR_Median_Value',\n",
       " 'NIR_Std_Dev_Value']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "std_columns = []\n",
    "std_columns_mean = []\n",
    "std_columns_median = []\n",
    "std_columns_stdev = []\n",
    "\n",
    "for key, value in find_cols.items():\n",
    "    if key in id_cols_var:\n",
    "        std_columns.append(value)\n",
    "        std_columns_mean.append(value)\n",
    "        std_columns_median.append(value)\n",
    "        std_columns_stdev.append(value)\n",
    "\n",
    "    else:\n",
    "        std_columns.extend([value+y for x, y in dtype_f.items()])\n",
    "        std_columns_mean.append(value+dtype_f[0])\n",
    "        std_columns_median.append(value+dtype_f[1])\n",
    "        std_columns_stdev.append(value+dtype_f[2])\n",
    "        \n",
    "std_columns.remove('Plot_ID')\n",
    "std_columns_mean.remove('Plot_ID')\n",
    "std_columns_median.remove('Plot_ID')\n",
    "std_columns_stdev.remove('Plot_ID')\n",
    "\n",
    "std_columns\n",
    "# std_columns_mean\n",
    "# std_columns_median\n",
    "# std_columns_stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:48:10.188490Z",
     "start_time": "2021-09-08T15:48:10.175325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'Plot_ID',\n",
       " 'plot': 'Plot_ID',\n",
       " 'Blue': 'Blue',\n",
       " 'Gre': 'Green',\n",
       " 'Red': 'Red',\n",
       " 'RedE': 'RedEdge',\n",
       " 'NIR': 'NIR'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find df which have all the required std columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:48:10.204449Z",
     "start_time": "2021-09-08T15:48:10.189488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 56 complete dataframes. Can be accessed using \"complete_dataframes\" list\n",
      "Found 7 incomplete dataframes. Can be accessed using \"incomplete_df\" list\n",
      "Wall time: 997 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "complete_dataframes = []\n",
    "incomplete_df = []\n",
    "\n",
    "for df in all_df:\n",
    "    df_columns = locals()[df].columns\n",
    "    if (all(x in df_columns for x in std_columns)):\n",
    "        complete_dataframes.append(df)\n",
    "    else:\n",
    "        incomplete_df.append(df)\n",
    "print(f'Found {len(complete_dataframes)} complete dataframes. Can be accessed using \\\"complete_dataframes\\\" list')\n",
    "print(f'Found {len(incomplete_df)} incomplete dataframes. Can be accessed using \\\"incomplete_df\\\" list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting complete and incomplete datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:48:12.068422Z",
     "start_time": "2021-09-08T15:48:10.205446Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exporting complete df\n",
    "# Defining path\n",
    "os.makedirs(complete_df_path, exist_ok=True)\n",
    "\n",
    "# Test: Check if there are duplicate names in the complete_dataframes list\n",
    "# As the last file with the same name will overwrite the previously exported file, resulting in data loss\n",
    "if len(complete_dataframes) != len(set(complete_dataframes)):\n",
    "    print('There are duplicate names in the complete_dataframes list.\\nPlease check before exporting as it might result in data loss.')\n",
    "        \n",
    "    # Printing the names of the duplicate datasets, if any\n",
    "    find_duplicates=[]\n",
    "    for i in complete_dataframes:\n",
    "        if i not in find_duplicates:\n",
    "            find_duplicates.append(i)\n",
    "        else:\n",
    "            print(f'Duplicate dataset named \\'{i}\\'')\n",
    "    raise NameError\n",
    "    \n",
    "for df in complete_dataframes:\n",
    "    locals()[df].to_csv(complete_df_path+df+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:48:12.431539Z",
     "start_time": "2021-09-08T15:48:12.071416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining path\n",
    "os.makedirs(incomplete_df_path, exist_ok=True)\n",
    "\n",
    "# Test: Check if there are duplicate names in the incomplete_df list\n",
    "# As the last file with the same name will overwrite the previously exported file, resulting in data loss\n",
    "if len(incomplete_df) != len(set(incomplete_df)):\n",
    "    print('There are duplicate names in the incomplete_df list.\\nPlease check before exporting as it might result in data loss.')\n",
    "    \n",
    "    # Printing the names of the duplicate datasets\n",
    "    find_duplicates=[]\n",
    "    for i in incomplete_df:\n",
    "        if i not in find_duplicates:\n",
    "            find_duplicates.append(i)\n",
    "        else:\n",
    "            print(f'Duplicate dataset named \\'{i}\\'')\n",
    "    raise NameError\n",
    "    \n",
    "for df in incomplete_df:\n",
    "    locals()[df].to_csv(incomplete_df_path+df+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-08T15:48:12.446926Z",
     "start_time": "2021-09-08T15:48:12.432329Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "a_file = open(\"Data\\std_columns.json\", \"w\")\n",
    "json.dump(std_columns, a_file)\n",
    "a_file.close()\n",
    "\n",
    "# a_file = open(\"Data\\std_columns.json\", \"r\")\n",
    "# output = a_file.read()\n",
    "# a_file.close()\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF SECTION"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "433.45px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
