{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "To do\n",
    "\n",
    "Check Time Series data\n",
    "\n",
    "Give a 2 page report to Sahameh about visualisation and indicate the data which has any problems, which is in consistant with the data from other dates\n",
    "Planting date for 2018 is given in master thesis\n",
    "Planting date for 2019 is May 05\n",
    "Planting date for 2020 and remaining data will be given by Sahameh in the week starrting from 16th November\n",
    "\n",
    "Must do normalisation of data before training\n",
    "\n",
    "Next step is Global Mix modelling\n",
    "Sahameh will share a paper from 2020 about using hyper spectral imaging, which will be basis of Mix Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning to predict Wheat Yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory with data files\n",
    "path = r\"C:\\Users\\fahad\\Documents\\Master Thesis\\Phenotyping\\Data_from_2017_and_2018\"\n",
    "path = path.replace(\"\\\\\", \"/\") + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Deep Learning-keras.ipynb',\n",
       " 'Graminor_2018_complete.csv',\n",
       " 'Masbasis_2017.xlsx',\n",
       " 'Masbasis_2018_yp.xlsx',\n",
       " 'Robot_2017.xlsx',\n",
       " 'Robot_2018_YP.xlsx']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_dir = os.listdir(path)\n",
    "list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graminor_18 = list_dir[2]\n",
    "masbasis_17 = list_dir[3]\n",
    "masbasis_18 = list_dir[4]\n",
    "robot_17 = list_dir[5]\n",
    "robot_18 = list_dir[6]\n",
    "\n",
    "data_file_list = [graminor_18, masbasis_17, masbasis_18, robot_17, robot_18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the name of the variable as string\n",
    "\n",
    "import inspect\n",
    "\n",
    "def retrieve_name(var):\n",
    "        \"\"\"\n",
    "        Gets the name of var. Does it from the out most frame inner-wards.\n",
    "        :param var: variable to get name from.\n",
    "        :return: string\n",
    "        \"\"\"\n",
    "        for fi in reversed(inspect.stack()):\n",
    "            names = [var_name for var_name, var_val in fi.frame.f_locals.items() if var_val is var]\n",
    "            if len(names) > 0:\n",
    "                return names[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": false,
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graminor_18_df\n",
      "masbasis_17_df\n",
      "masbasis_18_df\n",
      "robot_17_14_06_17_df\n",
      "robot_17_19_06_17_df\n",
      "robot_17_29_06_17_df\n",
      "robot_17_03_07_17_df\n",
      "robot_17_14_07_17_df\n",
      "robot_17_17_07_17_df\n",
      "robot_17_14_08_17_df\n",
      "robot_18_02_07_18_df\n",
      "robot_18_smallfield05_07_18_df\n",
      "robot_18_smallfield11_07_18_df\n",
      "robot_18_smallfield19_07_18_df\n",
      "robot_18_smallfield24_07_18_df\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "df_str_list = []\n",
    "\n",
    "for filename in data_file_list:\n",
    "    new_df = retrieve_name(filename) + '_df'\n",
    "    if filename[-3:] == 'csv':\n",
    "        locals()[new_df] = pd.read_csv(path + filename,\n",
    "                                       delimiter=';',\n",
    "                                       encoding=\"ISO-8859-1\")\n",
    "        df_str_list.append(new_df)\n",
    "        # Encoding has been defined to avoid UnicodeDecodeError while reading the csv\n",
    "#         display(locals()[new_df].head(10))    # Print the df in pretty format\n",
    "    elif filename[-4:] == 'xlsx':\n",
    "        locals()[new_df] = pd.ExcelFile(path + filename)\n",
    "        if len(locals()[new_df].sheet_names) == 1:\n",
    "            locals()[new_df] = pd.read_excel(path + filename)\n",
    "            df_str_list.append(new_df)\n",
    "        elif len(locals()[new_df].sheet_names) > 1:\n",
    "            for i in locals()[new_df].sheet_names:\n",
    "                # Replacing '.' and '-' in the sheet file name to '_'\n",
    "                k = i.replace('.', '_')\n",
    "                k = k.replace('-', '_')\n",
    "                new_sheet_df = retrieve_name(filename) + '_' + k + '_df'\n",
    "                locals()[new_sheet_df] = pd.read_excel(path + filename, sheet_name = i)\n",
    "                df_str_list.append(new_sheet_df)\n",
    "\n",
    "#         display(locals()[new_df].head(10))    # Print the df in pretty format\n",
    "\n",
    "print(*df_str_list, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "graminor_18_df_median_columns\n",
      "(599, 109)\n",
      "26-06-18_bluemedian\n",
      "26-06-18_greenmedian\n",
      "26-06-18_ndvimedian\n",
      "26-06-18_nirmedian\n",
      "26-06-18_redmedian\n",
      "26-06-18_rededgemedian\n",
      "26-06-18_mtci\n",
      "MAT\n",
      "26-06-18_evi\n",
      "02-07-18_bluemedian\n",
      "02-07-18_greenmedian\n",
      "02-07-18_ndvimedian\n",
      "02-07-18_nirmedian\n",
      "02-07-18_redmedian\n",
      "02-07-18_rededgemedian\n",
      "02-07-18_mtci\n",
      "02-07-18MAT\n",
      "02-07-18_evi\n",
      "19-07-18_bluemedian\n",
      "19-07-18_greenmedian\n",
      "19-07-18_ndvimedian\n",
      "19-07-18_nirmedian\n",
      "19-07-18_redmedian\n",
      "19-07-18_rededgemedian\n",
      "19-07-18_mtci\n",
      "19-07-18MAT\n",
      "19-07-18_evi\n",
      "02-07-18_mtci - 26-06-18_mtci\n",
      "19-07-18_mtci - 26-06-18_mtci\n",
      "\n",
      "\n",
      "masbasis_17_df_median_columns\n",
      "(560, 179)\n",
      "14-07-17_greenmedian\n",
      "14-07-17_ndvimedian\n",
      "14-07-17_nirmedian\n",
      "14-07-17_redmedian\n",
      "14-07-17_rededgemedian\n",
      "14-07-17_mtci\n",
      "MAT\n",
      "17-07-17_greenmedian\n",
      "17-07-17_ndvimedian\n",
      "17-07-17_nirmedian\n",
      "17-07-17_redmedian\n",
      "17-07-17_rededgemedian\n",
      "17-07-17_mtci\n",
      "17-07-17MAT\n",
      "20-07-17_greenmedian\n",
      "20-07-17_ndvimedian\n",
      "20-07-17_nirmedian\n",
      "20-07-17_redmedian\n",
      "20-07-17_rededgemedian\n",
      "20-07-17_mtci\n",
      "20-07-17MAT\n",
      "01-08-17_greenmedian\n",
      "01-08-17_ndvimedian\n",
      "01-08-17_nirmedian\n",
      "01-08-17_redmedian\n",
      "01-08-17_rededgemedian\n",
      "01-08-17_mtci\n",
      "01-08-17MAT\n",
      "MAT.1\n",
      "17-07-17_mtci - 14-07-17_mtci\n",
      "20-07-17_mtci - 14-07-17_mtci\n",
      "01-08-17_mtci - 14-07-17_mtci\n",
      "\n",
      "\n",
      "masbasis_18_df_median_columns\n",
      "(528, 101)\n",
      "MAT\n",
      "13-07-18_bluemedian\n",
      "13-07-18_evi\n",
      "13-07-18_greenmedian\n",
      "13-07-18_mtci\n",
      "13-07-18_ndvimedian\n",
      "13-07-18_nirmedian\n",
      "13-07-18_rededgemedian\n",
      "13-07-18_redmedian\n",
      "26-07-18_bluemedian\n",
      "26-07-18_greenmedian\n",
      "26-07-18_ndvimedian\n",
      "26-07-18_nirmedian\n",
      "26-07-18_redmedian\n",
      "26-07-18_rededgemedian\n",
      "26-07-18_mtci\n",
      "26-07-18MAT\n",
      "26-07-18_evi\n",
      "26-07-18_mtci - 13-07-18_mtci\n",
      "\n",
      "\n",
      "robot_17_14_06_17_df_median_columns\n",
      "(96, 33)\n",
      "_greenmedian\n",
      "_ndvimedian\n",
      "_nirmedian\n",
      "_redmedian\n",
      "_rededgemedian\n",
      "_mtci\n",
      "\n",
      "\n",
      "robot_17_19_06_17_df_median_columns\n",
      "(96, 73)\n",
      "_greenmedian\n",
      "_ndvimedian\n",
      "_nirmedian\n",
      "_redmedian\n",
      "_rededgemedian\n",
      "_mtci\n",
      "\n",
      "\n",
      "robot_17_29_06_17_df_median_columns\n",
      "(96, 73)\n",
      "_greenmedian\n",
      "_ndvimedian\n",
      "_nirmedian\n",
      "_redmedian\n",
      "_rededgemedian\n",
      "_mtci\n",
      "\n",
      "\n",
      "robot_17_03_07_17_df_median_columns\n",
      "(96, 73)\n",
      "_greenmedian\n",
      "_ndvimedian\n",
      "_nirmedian\n",
      "_redmedian\n",
      "_rededgemedian\n",
      "_mtci\n",
      "\n",
      "\n",
      "robot_17_14_07_17_df_median_columns\n",
      "(96, 73)\n",
      "_greenmedian\n",
      "_ndvimedian\n",
      "_nirmedian\n",
      "_redmedian\n",
      "_rededgemedian\n",
      "_mtci\n",
      "\n",
      "\n",
      "robot_17_17_07_17_df_median_columns\n",
      "(96, 73)\n",
      "_greenmedian\n",
      "_ndvimedian\n",
      "_nirmedian\n",
      "_redmedian\n",
      "_rededgemedian\n",
      "_mtci\n",
      "\n",
      "\n",
      "robot_17_14_08_17_df_median_columns\n",
      "(96, 73)\n",
      "_greenmedian\n",
      "_ndvimedian\n",
      "_nirmedian\n",
      "_redmedian\n",
      "_rededgemedian\n",
      "_mtci\n",
      "\n",
      "\n",
      "robot_18_02_07_18_df_median_columns\n",
      "(96, 22)\n",
      "_bluemedian\n",
      "_greenmedian\n",
      "_ndvimedian\n",
      "_nirmedian\n",
      "_redmedian\n",
      "_rededgemedian\n",
      "_mtci\n",
      "_evi\n",
      "\n",
      "\n",
      "robot_18_smallfield05_07_18_df_median_columns\n",
      "(96, 22)\n",
      "_bluemedian\n",
      "_greenmedian\n",
      "_ndvimedian\n",
      "_nirmedian\n",
      "_redmedian\n",
      "_rededgemedian\n",
      "_mtci\n",
      "_evi\n",
      "\n",
      "\n",
      "robot_18_smallfield11_07_18_df_median_columns\n",
      "(96, 22)\n",
      "_bluemedian\n",
      "_greenmedian\n",
      "_ndvimedian\n",
      "_nirmedian\n",
      "_redmedian\n",
      "_rededgemedian\n",
      "_mtci\n",
      "_evi\n",
      "\n",
      "\n",
      "robot_18_smallfield19_07_18_df_median_columns\n",
      "(96, 28)\n",
      "_bluemedian\n",
      "_greenmedian\n",
      "_ndvimedian\n",
      "_nirmedian\n",
      "_redmedian\n",
      "_rededgemedian\n",
      "_mtci\n",
      "_evi\n",
      "\n",
      "\n",
      "robot_18_smallfield24_07_18_df_median_columns\n",
      "(96, 30)\n",
      "_bluemedian\n",
      "_greenmedian\n",
      "_ndvimedian\n",
      "_nirmedian\n",
      "_redmedian\n",
      "_rededgemedian\n",
      "_mtci\n",
      "_evi\n"
     ]
    }
   ],
   "source": [
    "lists_of_median_cols = []\n",
    "\n",
    "for df in df_str_list:\n",
    "    list_of_columns = df+'_median_columns'\n",
    "    locals()[list_of_columns] = []\n",
    "    for strg in locals()[df].columns.tolist():\n",
    "        if strg.find('median') != -1 or strg.find('mtci') != -1 or strg.find('evi') != -1 or strg.find('MAT') != -1:\n",
    "            locals()[list_of_columns].append(strg)\n",
    "    lists_of_median_cols.append(list_of_columns)\n",
    "    \n",
    "    print('\\n', list_of_columns,locals() [df].shape, *locals()[list_of_columns], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "graminor_18_df_yield_columns\n",
      "(599, 109)\n",
      "26-06-18GrainYield\n",
      "02-07-18GrainYield\n",
      "19-07-18GrainYield\n",
      "\n",
      "\n",
      "masbasis_17_df_yield_columns\n",
      "(560, 179)\n",
      "14-07-17Yield\n",
      "17-07-17Yield\n",
      "20-07-17Yield\n",
      "01-08-17Yield\n",
      "\n",
      "\n",
      "masbasis_18_df_yield_columns\n",
      "(528, 101)\n",
      "13-07-18GrainYield\n",
      "26-07-18GrainYield\n",
      "\n",
      "\n",
      "robot_17_14_06_17_df_yield_columns\n",
      "(96, 33)\n",
      "GrainYield\n",
      "HarvestYield \n",
      "\n",
      "\n",
      "robot_17_19_06_17_df_yield_columns\n",
      "(96, 73)\n",
      "GrainYield\n",
      "HarvestYield \n",
      "\n",
      "\n",
      "robot_17_29_06_17_df_yield_columns\n",
      "(96, 73)\n",
      "GrainYield\n",
      "HarvestYield \n",
      "\n",
      "\n",
      "robot_17_03_07_17_df_yield_columns\n",
      "(96, 73)\n",
      "GrainYield\n",
      "HarvestYield \n",
      "\n",
      "\n",
      "robot_17_14_07_17_df_yield_columns\n",
      "(96, 73)\n",
      "GrainYield\n",
      "HarvestYield \n",
      "\n",
      "\n",
      "robot_17_17_07_17_df_yield_columns\n",
      "(96, 73)\n",
      "GrainYield\n",
      "HarvestYield \n",
      "\n",
      "\n",
      "robot_17_14_08_17_df_yield_columns\n",
      "(96, 73)\n",
      "GrainYield\n",
      "HarvestYield \n",
      "\n",
      "\n",
      "robot_18_02_07_18_df_yield_columns\n",
      "(96, 22)\n",
      "GrainYield\n",
      "\n",
      "\n",
      "robot_18_smallfield05_07_18_df_yield_columns\n",
      "(96, 22)\n",
      "GrainYield\n",
      "\n",
      "\n",
      "robot_18_smallfield11_07_18_df_yield_columns\n",
      "(96, 22)\n",
      "GrainYield\n",
      "\n",
      "\n",
      "robot_18_smallfield19_07_18_df_yield_columns\n",
      "(96, 28)\n",
      "GrainYield\n",
      "\n",
      "\n",
      "robot_18_smallfield24_07_18_df_yield_columns\n",
      "(96, 30)\n",
      "GrainYield\n"
     ]
    }
   ],
   "source": [
    "lists_of_yield_cols = []\n",
    "\n",
    "for df in df_str_list:\n",
    "    list_of_columns = df+'_yield_columns'\n",
    "    locals()[list_of_columns] = []\n",
    "    for strg in locals()[df].columns.tolist():\n",
    "        if strg.find('Yield') != -1:\n",
    "            locals()[list_of_columns].append(strg)\n",
    "    lists_of_yield_cols.append(list_of_columns)\n",
    "    \n",
    "    print('\\n', list_of_columns,locals() [df].shape, *locals()[list_of_columns], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GrainYield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>195.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>223.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>217.483333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155.716667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>283.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>269.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>278.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>337.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>349.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    GrainYield\n",
       "0   195.033333\n",
       "1   223.100000\n",
       "2   217.483333\n",
       "3   188.750000\n",
       "4   155.716667\n",
       "..         ...\n",
       "91  283.083333\n",
       "92  269.700000\n",
       "93  278.150000\n",
       "94  337.233333\n",
       "95  349.916667\n",
       "\n",
       "[96 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robot_18_smallfield24_07_18_df[robot_18_smallfield24_07_18_df_yield_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Split dataframe into data and target\n",
    "#==============================================================================\n",
    "\n",
    "X = robot_18_smallfield24_07_18_df[\n",
    "    robot_18_smallfield24_07_18_df_median_columns]\n",
    "y = robot_18_smallfield24_07_18_df[\n",
    "    robot_18_smallfield24_07_18_df_yield_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Create separate train/test splits from Main data\n",
    "#==============================================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Scale features using StandardScaler class in scikit-learn\n",
    "#==============================================================================\n",
    "\n",
    "# Initialise standard scaler and compute mean and STD from training data\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "# Transform (standardise) both X_train and X_test with mean and STD from\n",
    "# training data\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Defining the function to vaiidate the model with the test data and\n",
    "# get the results from regression evaluation metrices in sklearn\n",
    "#==============================================================================\n",
    "pred = []\n",
    "accuracy = []\n",
    "\n",
    "\n",
    "def test_data_regression(model, X_test, y_test):\n",
    "    pred = []\n",
    "    accuracy = []\n",
    "    #==============================================================================\n",
    "    # Make predictions for test set\n",
    "    #==============================================================================\n",
    "\n",
    "    # Predict classes for samples in test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #==============================================================================\n",
    "    # Compute performance\n",
    "    #==============================================================================\n",
    "\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    print(mae, ' mean_absolute_error')\n",
    "    accuracy.append(mae)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mse = mean_squared_error(y_test, y_pred, squared=True)\n",
    "    print(mse, ' mean_squared_error')\n",
    "    accuracy.append(mse)\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(rmse, ' root_mean_squared_error')\n",
    "    accuracy.append(rmse)\n",
    "\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(r2, ' r2_score')\n",
    "    accuracy.append(r2)\n",
    "\n",
    "    # Print accuracy computed from predictions on the test set\n",
    "    print(accuracy)\n",
    "\n",
    "    #==============================================================================\n",
    "    # Append Results\n",
    "    #==============================================================================\n",
    "    results = []\n",
    "    import datetime\n",
    "    datetime = datetime.datetime.now()\n",
    "    results.append((model, 'MAE = {}'.format(mae), 'MSE = {}'.format(mse),\n",
    "                    'RMSE = {}'.format(rmse), 'R2 = {}'.format(r2),\n",
    "                    'List = {}'.format(accuracy), datetime))\n",
    "\n",
    "    pd.DataFrame(np.asarray(results)).to_csv('results.csv',\n",
    "                                             mode='a',\n",
    "                                             header=None)\n",
    "    pred.extend(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.791968968083356  mean_absolute_error\n",
      "1510.5850790239026  mean_squared_error\n",
      "38.86624601146736  root_mean_squared_error\n",
      "0.476003836137754  r2_score\n",
      "[31.791968968083356, 1510.5850790239026, 38.86624601146736, 0.476003836137754]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=1000,\n",
    "                              max_depth=250,\n",
    "                              min_samples_split=5,\n",
    "                              random_state=0,\n",
    "                              n_jobs=-1)\n",
    "model.fit(X_train_std, y_train)\n",
    "y_pred = model.predict(X_test_std)\n",
    "test_data_regression(model, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.069610149758933  mean_absolute_error\n",
      "1146.6873396491621  mean_squared_error\n",
      "33.862772179034046  root_mean_squared_error\n",
      "0.6022337467322105  r2_score\n",
      "[26.069610149758933, 1146.6873396491621, 33.862772179034046, 0.6022337467322105]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "model = PLSRegression(n_components=5)\n",
    "model.fit(X_train_std, y_train)\n",
    "y_pred = model.predict(X_test_std)\n",
    "test_data_regression(model, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[228.43445511],\n",
       "       [208.16495241],\n",
       "       [229.30338632],\n",
       "       [146.42192367],\n",
       "       [175.99016696],\n",
       "       [265.87055526],\n",
       "       [182.72079741],\n",
       "       [150.79555018],\n",
       "       [179.94328825],\n",
       "       [177.0734073 ],\n",
       "       [172.45244835],\n",
       "       [253.03559614],\n",
       "       [197.43453425],\n",
       "       [222.45180587],\n",
       "       [181.9630764 ],\n",
       "       [176.08251618],\n",
       "       [193.21170533],\n",
       "       [208.14538433],\n",
       "       [178.39723128],\n",
       "       [225.7900946 ],\n",
       "       [189.53237185],\n",
       "       [183.57667404],\n",
       "       [154.35145008],\n",
       "       [213.56089343],\n",
       "       [139.02729324],\n",
       "       [191.62636537],\n",
       "       [144.79200749],\n",
       "       [190.33373498],\n",
       "       [209.64883211]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import cv2\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Neural networks\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, Conv2DTranspose, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Training\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define eval metric and loss function (DICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Metric used in the competition\n",
    "\"\"\" \n",
    "# Defining the dice_coef function\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "# Defining the dice_loss function\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Create U-net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Version of U-Net with dropout and size preservation (padding= 'same')\n",
    "\"\"\" \n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def get_deep_unet(input_img, n_filters = 16, dropout = 0.1, batchnorm = True, n_classes = 1, growth_factor=2, upconv=True):\n",
    "\n",
    "    inputs = input_img\n",
    "    #inputs = BatchNormalization()(inputs)\n",
    "    \n",
    "    # Creating deep convnets using Conv2D and MaxPooling2D in each convolutional layer\n",
    "    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    #pool1 = Dropout(droprate)(pool1)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    # Applying batch normalization to each layer to be able to use deep convnets\n",
    "    pool1 = BatchNormalization()(pool1)\n",
    "    # Second Covnet\n",
    "    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(dropout)(pool2)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    # Applying batch normalization\n",
    "    pool2 = BatchNormalization()(pool2)\n",
    "    # Third Covnet\n",
    "    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(dropout)(pool3)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    # Applying batch normalization\n",
    "    pool3 = BatchNormalization()(pool3)\n",
    "    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_0)\n",
    "    pool4_1 = MaxPooling2D(pool_size=(2, 2))(conv4_0)\n",
    "    pool4_1 = Dropout(dropout)(pool4_1)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    # Applying batch normalization\n",
    "    pool4_1 = BatchNormalization()(pool4_1)\n",
    "    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_1)\n",
    "    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_1)\n",
    "    pool4_2 = MaxPooling2D(pool_size=(2, 2))(conv4_1)\n",
    "    pool4_2 = Dropout(dropout)(pool4_2)\n",
    "\n",
    "    n_filters *= growth_factor\n",
    "    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_2)\n",
    "    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    n_filters //= growth_factor\n",
    "    # Using image upsampling by either Conv2DTranspose or UpSampling2D\n",
    "    if upconv:\n",
    "        up6_1 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv5), conv4_1])\n",
    "    else:\n",
    "        up6_1 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4_1])\n",
    "    # Applying batch normalization\n",
    "    up6_1 = BatchNormalization()(up6_1)\n",
    "    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_1)\n",
    "    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_1)\n",
    "    conv6_1 = Dropout(dropout)(conv6_1)\n",
    "\n",
    "    # Using image upsampling by either Conv2DTranspose or UpSampling2D\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up6_2 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_1), conv4_0])\n",
    "    else:\n",
    "        up6_2 = concatenate([UpSampling2D(size=(2, 2))(conv6_1), conv4_0])\n",
    "    up6_2 = BatchNormalization()(up6_2)\n",
    "    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_2)\n",
    "    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_2)\n",
    "    conv6_2 = Dropout(dropout)(conv6_2)\n",
    "\n",
    "    # Using image upsampling by either Conv2DTranspose or UpSampling2D\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up7 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_2), conv3])\n",
    "    else:\n",
    "        up7 = concatenate([UpSampling2D(size=(2, 2))(conv6_2), conv3])\n",
    "    # Applying batch normalization\n",
    "    up7 = BatchNormalization()(up7)\n",
    "    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = Dropout(dropout)(conv7)\n",
    "\n",
    "    # Using image upsampling by either Conv2DTranspose or UpSampling2D\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up8 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv7), conv2])\n",
    "    else:\n",
    "        up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2])\n",
    "    # Applying batch normalization\n",
    "    up8 = BatchNormalization()(up8)\n",
    "    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = Dropout(dropout)(conv8)\n",
    "\n",
    "    # Using image upsampling by either Conv2DTranspose or UpSampling2D\n",
    "    n_filters //= growth_factor\n",
    "    if upconv:\n",
    "        up9 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv8), conv1])\n",
    "    else:\n",
    "        up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1])\n",
    "    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(n_classes, (1, 1), activation='sigmoid')(conv9)\n",
    "    \n",
    "    # Creating model and feeding the input image and the final convolutional layer into it\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input to `.fit()` should have rank 4. Got array with shape: (67, 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-639feb388496>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Fit DataGen instance to X_train (Only needed for standardization etc.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtrain_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Create flow for training images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\image_data_generator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, augment, rounds, seed)\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m             raise ValueError('Input to `.fit()` should have rank 4. '\n\u001b[1;32m--> 936\u001b[1;33m                              'Got array with shape: ' + str(x.shape))\n\u001b[0m\u001b[0;32m    937\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchannel_axis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m             warnings.warn(\n",
      "\u001b[1;31mValueError\u001b[0m: Input to `.fit()` should have rank 4. Got array with shape: (67, 8)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data augmentation for training dataset\n",
    "\"\"\"\n",
    "\n",
    "# Define batch size for augmentation\n",
    "batch_size = 64\n",
    "# Define random seed\n",
    "seed = 123\n",
    "\n",
    "# Dict containing augmentation variables\n",
    "data_gen_args = dict(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    rotation_range=90,\n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "# Instantiate DataGenerator class with variables from dict\n",
    "train_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Fit DataGen instance to X_train (Only needed for standardization etc.)\n",
    "train_datagen.fit(X_train)\n",
    "\n",
    "# Create flow for training images\n",
    "image_train_generator = train_datagen.flow(\n",
    "    X_train,\n",
    "    None,\n",
    "    seed=seed,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# Create separate flow for training masks\n",
    "mask_train_generator = train_datagen.flow(\n",
    "    y_train,\n",
    "    None,\n",
    "    seed=seed,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# Combine generators into one which yields image and masks\n",
    "train_generator = zip(image_train_generator, mask_train_generator)\n",
    "\"\"\"\n",
    "Model creation and compilation\n",
    "\"\"\"\n",
    "\n",
    "# Define input class with correct shape\n",
    "input_img = Input(shape=(128, 128, 4))\n",
    "\n",
    "# Number of filters in the U-net\n",
    "n_filters = 64\n",
    "\n",
    "# Instantiate U-net without dropout with batchnorm\n",
    "model = get_deep_unet(input_img,\n",
    "                      n_filters=n_filters,\n",
    "                      dropout=0.0,\n",
    "                      batchnorm=True,\n",
    "                      n_classes=1)\n",
    "\n",
    "# Compile model using \"Adam\" with custom loss function and metrics\n",
    "model.compile(optimizer='adam', loss=dice_loss, metrics=[dice_coef])\n",
    "\"\"\"\n",
    "Callbacks\n",
    "\"\"\"\n",
    "\n",
    "# Early stopping for saving compute time\n",
    "earlystopping = EarlyStopping(monitor='val_dice_coef',\n",
    "                              verbose=1,\n",
    "                              min_delta=0.01,\n",
    "                              patience=3,\n",
    "                              mode='max')\n",
    "\n",
    "# Saving best models\n",
    "model_path = '/content/drive/My Drive/Skole/CA2/tmp/unet.model'\n",
    "checkpoint = ModelCheckpoint(model_path,\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1,\n",
    "                             mode='min')\n",
    "\n",
    "# Reduce learning rate to improve accuracy on plateaus\n",
    "redlr = ReduceLROnPlateau(factor=0.1,\n",
    "                          patience=1,\n",
    "                          min_lr=1e-08,\n",
    "                          verbose=1,\n",
    "                          monitor='val_loss',\n",
    "                          mode='min'),\n",
    "\n",
    "# Collect callbacks in list\n",
    "callbacks_list = [redlr, earlystopping, checkpoint]\n",
    "\"\"\"\n",
    "Model training\n",
    "\"\"\"\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=(X_train.shape[0] // n_filters),\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    validation_steps=(X_val.shape[0] // n_filters),\n",
    "                    epochs=4,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
